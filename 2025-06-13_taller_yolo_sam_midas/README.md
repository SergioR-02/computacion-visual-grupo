# ðŸ” Pipeline de VisiÃ³n Computacional: DetecciÃ³n, SegmentaciÃ³n y EstimaciÃ³n de Profundidad

ðŸ“… **Fecha:** 2025-01-07 â€“ Fecha de realizaciÃ³n

ðŸŽ¯ **Objetivo del Proyecto:**
Desarrollar un pipeline integrado de visiÃ³n computacional que combine detecciÃ³n de objetos con YOLO, segmentaciÃ³n con SAM (Segment Anything Model), y estimaciÃ³n de profundidad con MiDaS. El objetivo es crear un sistema completo que detecte objetos, los segmente con precisiÃ³n, estime su profundidad, y genere visualizaciones y aplicaciones creativas como efectos de bokeh y anÃ¡lisis de datos.

---

## ðŸ§  Conceptos Aprendidos

### ðŸŽ¯ DetecciÃ³n de Objetos con YOLO

- **DefiniciÃ³n**: YOLO (You Only Look Once) es un algoritmo de detecciÃ³n de objetos en tiempo real que divide la imagen en una cuadrÃ­cula y predice bounding boxes y probabilidades de clase
- **CaracterÃ­sticas principales**:
  - âœ… DetecciÃ³n en una sola pasada de la red neuronal
  - âœ… PredicciÃ³n simultÃ¡nea de mÃºltiples objetos
  - âœ… Salida con coordenadas de bounding boxes y confianza
  - âœ… Soporte para mÃºltiples clases de objetos (80+ en COCO dataset)

### âœ‚ï¸ SegmentaciÃ³n con SAM (Segment Anything Model)

- **Prompt-based Segmentation**: SAM utiliza prompts (puntos, boxes, texto) para generar mÃ¡scaras de segmentaciÃ³n precisas
- **Box Prompting**: Uso de bounding boxes de YOLO como prompts para SAM
- **Mask Generation**: ProducciÃ³n de mÃ¡scaras binarias de alta calidad
- **Zero-shot Capability**: Capacidad de segmentar objetos sin entrenamiento especÃ­fico

### ðŸ“ EstimaciÃ³n de Profundidad con MiDaS

- **Monocular Depth Estimation**: EstimaciÃ³n de profundidad a partir de una sola imagen
- **Relative Depth**: MiDaS produce mapas de profundidad relativa (no absoluta)
- **Multi-scale Processing**: Procesamiento a diferentes escalas para mejor precisiÃ³n
- **Dense Prediction**: Cada pÃ­xel recibe una estimaciÃ³n de profundidad

### ðŸŽ¨ Aplicaciones Creativas y AnalÃ­ticas

- **Efecto Bokeh**: Desenfoque selectivo basado en profundidad
- **Pixelado de Fondo**: AplicaciÃ³n de efectos a regiones no segmentadas
- **AnÃ¡lisis EstadÃ­stico**: ExtracciÃ³n de mÃ©tricas de Ã¡rea, profundidad y distribuciÃ³n
- **ExportaciÃ³n de Datos**: GeneraciÃ³n de reportes en formato CSV

---

## ðŸ”§ Herramientas y Entornos

- **Python 3.8+** - Lenguaje de programaciÃ³n principal
- **PyTorch** - Framework de deep learning para YOLO y SAM
- **OpenCV** - Procesamiento de imÃ¡genes y visualizaciÃ³n
- **Transformers** - Modelos pre-entrenados de Hugging Face
- **NumPy** - Operaciones numÃ©ricas y manipulaciÃ³n de arrays
- **Matplotlib** - VisualizaciÃ³n de resultados y grÃ¡ficos
- **Pillow (PIL)** - Procesamiento adicional de imÃ¡genes
- **Pandas** - AnÃ¡lisis y exportaciÃ³n de datos
- **Google Colab** - Entorno de desarrollo en la nube con GPU
---

## ðŸ“ Estructura del Proyecto

```
Pipeline_Vision_Computacional/
â”œâ”€â”€ deteccion_objetos_yolo.ipynb     # Notebook principal del pipeline
â”œâ”€â”€ README.md                        # Esta documentaciÃ³n
â”œâ”€â”€ informeParte1.md  
â””â”€â”€ resultados/                      # ImÃ¡genes generadas y anÃ¡lisis
```

---

## ðŸ§ª ImplementaciÃ³n

### ðŸ”¹ Etapas realizadas

1. **ConfiguraciÃ³n del entorno**: InstalaciÃ³n automÃ¡tica de dependencias para Colab y local
2. **DetecciÃ³n con YOLO**: ImplementaciÃ³n de YOLOv8 para detectar mÃºltiples clases de objetos
3. **SegmentaciÃ³n con SAM**: Uso de bounding boxes como prompts para generar mÃ¡scaras precisas
4. **EstimaciÃ³n de profundidad**: IntegraciÃ³n de MiDaS para mapas de profundidad densos
5. **VisualizaciÃ³n combinada**: Overlay de detecciones, mÃ¡scaras y profundidad
6. **Aplicaciones creativas**: Efectos de bokeh y pixelado basados en segmentaciÃ³n
7. **AnÃ¡lisis de datos**: ExtracciÃ³n de mÃ©tricas y exportaciÃ³n a CSV

### ðŸ”¹ Funciones principales implementadas

**DetecciÃ³n de objetos con YOLO:**

```python
def detectar_objetos_yolo(imagen_path, modelo_yolo, umbral_confianza=0.5):
    """
    Detecta objetos en una imagen usando YOLO.
    
    Args:
        imagen_path: Ruta a la imagen
        modelo_yolo: Modelo YOLO pre-entrenado
        umbral_confianza: Umbral mÃ­nimo de confianza
    
    Returns:
        tuple: (imagen_original, detecciones, boxes, clases, confianzas)
    """
    imagen = cv2.imread(imagen_path)
    imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)
    
    resultados = modelo_yolo(imagen_rgb, conf=umbral_confianza)
    
    # ExtracciÃ³n de bounding boxes, clases y confianzas
    for resultado in resultados:
        boxes = resultado.boxes.xyxy.cpu().numpy()
        clases = resultado.boxes.cls.cpu().numpy()
        confianzas = resultado.boxes.conf.cpu().numpy()
    
    return imagen_rgb, resultados, boxes, clases, confianzas
```

**SegmentaciÃ³n con SAM usando prompts de YOLO:**

```python
def segmentar_con_sam(imagen, boxes, predictor_sam):
    """
    Realiza segmentaciÃ³n usando SAM con bounding boxes como prompts.
    
    Args:
        imagen: Imagen de entrada (numpy array)
        boxes: Bounding boxes de YOLO
        predictor_sam: Predictor de SAM
    
    Returns:
        list: Lista de mÃ¡scaras de segmentaciÃ³n
    """
    predictor_sam.set_image(imagen)
    mascaras = []
    
    for box in boxes:
        # Convertir box a formato SAM (xyxy)
        input_box = np.array(box).astype(np.int32)
        
        # Generar mÃ¡scara usando el box como prompt
        masks, scores, logits = predictor_sam.predict(
            point_coords=None,
            point_labels=None,
            box=input_box[None, :],
            multimask_output=False,
        )
        
        mascaras.append(masks[0])
    
    return mascaras
```

**EstimaciÃ³n de profundidad con MiDaS:**

```python
def estimar_profundidad_midas(imagen, modelo_midas, transform_midas, device):
    """
    Estima la profundidad de una imagen usando MiDaS.
    
    Args:
        imagen: Imagen de entrada
        modelo_midas: Modelo MiDaS pre-entrenado
        transform_midas: Transformaciones para MiDaS
        device: Dispositivo (CPU/GPU)
    
    Returns:
        numpy.ndarray: Mapa de profundidad normalizado
    """
    # Preprocesar imagen para MiDaS
    input_batch = transform_midas(imagen).to(device)
    
    with torch.no_grad():
        prediction = modelo_midas(input_batch)
        
        # Interpolar a tamaÃ±o original
        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=imagen.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze()
    
    # Normalizar a rango [0, 1]
    depth_map = prediction.cpu().numpy()
    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
    
    return depth_map
```

**Efecto bokeh basado en profundidad:**

```python
def efecto_bokeh_profundidad(imagen, mapa_profundidad, mascaras, intensidad_blur=15):
    """
    Aplica efecto bokeh basado en profundidad y segmentaciÃ³n.
    
    Args:
        imagen: Imagen original
        mapa_profundidad: Mapa de profundidad
        mascaras: MÃ¡scaras de segmentaciÃ³n
        intensidad_blur: Intensidad del desenfoque
    
    Returns:
        numpy.ndarray: Imagen con efecto bokeh
    """
    imagen_bokeh = imagen.copy()
    
    # Crear mÃ¡scara combinada de objetos detectados
    mascara_objetos = np.zeros(imagen.shape[:2], dtype=bool)
    for mascara in mascaras:
        mascara_objetos |= mascara
    
    # Aplicar blur variable basado en profundidad
    imagen_blur = cv2.GaussianBlur(imagen, (intensidad_blur, intensidad_blur), 0)
    
    # Mezclar imagen original y blur basado en profundidad y segmentaciÃ³n
    for y in range(imagen.shape[0]):
        for x in range(imagen.shape[1]):
            if not mascara_objetos[y, x]:  # Fondo
                factor_blur = mapa_profundidad[y, x]
                alpha = np.clip(factor_blur * 1.5, 0, 1)
                imagen_bokeh[y, x] = (1 - alpha) * imagen[y, x] + alpha * imagen_blur[y, x]
    
    return imagen_bokeh.astype(np.uint8)
```

---

## ðŸ“Š Resultados y AnÃ¡lisis

### ðŸ“Œ Imagen Original
![Pipeline de VisiÃ³n Computacional](./resultados/Origin.png)
### ðŸ“Œ Pruebas YOLO

![Pipeline de VisiÃ³n Computacional](./resultados/DetallesYolo.png)
![Pipeline de VisiÃ³n Computacional](./resultados/DeteccionYolo.png)
![Pipeline de VisiÃ³n Computacional](./resultados/DistribucionYolo.png)

- âœ… **PrecisiÃ³n**: DetecciÃ³n exitosa de mÃºltiples clases (personas, vehÃ­culos, objetos)
- âœ… **Bounding Boxes**: Coordenadas precisas 

#### Multiples Objetos
![Pipeline de VisiÃ³n Computacional](./resultados/Cocina.png)


### ðŸ“Œ Pipeline Completo en AcciÃ³n:

![Pipeline de VisiÃ³n Computacional](./resultados/DiagramaPipeline.png)

#### RESULTADO YOLO SAM MiDas
![Pipeline de VisiÃ³n Computacional](./resultados/DiagramaPipeline.png)


![Pipeline de VisiÃ³n Computacional](./resultados/YoloSam.png)
### ðŸŽ¯ Detecciones YOLO:
- âœ… **PrecisiÃ³n**: DetecciÃ³n exitosa de mÃºltiples clases (personas, vehÃ­culos, objetos)
- âœ… **Bounding Boxes**: Coordenadas precisas para usar como prompts SAM
- âœ… **Confianza**: Filtrado por umbral de confianza configurable
- âœ… **Clases**: Soporte para 80+ clases del dataset COCO

![Pipeline de VisiÃ³n Computacional](./resultados/Galeria.png)
### âœ‚ï¸ SegmentaciÃ³n SAM:

- âœ… **MÃ¡scaras precisas**: SegmentaciÃ³n pixel-perfect usando prompts de YOLO
- âœ… **MÃºltiples objetos**: Procesamiento simultÃ¡neo de varios objetos detectados
- âœ… **Alta calidad**: Bordes suaves y precisos en las mÃ¡scaras
- âœ… **Robustez**: Funciona bien con objetos parcialmente ocluidos


![Pipeline de VisiÃ³n Computacional](./resultados/Profundidad.png)
### ðŸ“ Mapas de Profundidad:

- âœ… **Profundidad relativa**: EstimaciÃ³n consistente de distancias relativas
- âœ… **ResoluciÃ³n completa**: Mapas densos para cada pÃ­xel
- âœ… **Coherencia espacial**: Transiciones suaves entre regiones
- âœ… **Compatibilidad**: Funciona con imÃ¡genes de cualquier resoluciÃ³n


![Pipeline de VisiÃ³n Computacional](./resultados/EfectosVisuales.png)
### ðŸŽ¨ Aplicaciones Creativas:

- âœ… **Efecto Bokeh**: Desenfoque realista basado en profundidad
- âœ… **Pixelado selectivo**: Efectos artÃ­sticos en fondo/objetos
- âœ… **Recortes precisos**: ExtracciÃ³n de objetos segmentados
- âœ… **GalerÃ­a interactiva**: VisualizaciÃ³n organizada de resultados



![Pipeline de VisiÃ³n Computacional](./resultados/Clasificacion.png)
### ðŸ“ˆ AnÃ¡lisis EstadÃ­stico:

- âœ… **MÃ©tricas de Ã¡rea**: CÃ¡lculo de Ã¡reas de objetos segmentados
- âœ… **AnÃ¡lisis de profundidad**: EstadÃ­sticas de distribuciÃ³n de profundidad
- âœ… **ExportaciÃ³n CSV**: Datos estructurados para anÃ¡lisis posterior
- âœ… **Visualizaciones**: GrÃ¡ficos de distribuciÃ³n y comparativas

---

## ðŸ”§ ConfiguraciÃ³n y Uso

### ðŸ Requisitos del Sistema

```bash
# InstalaciÃ³n automÃ¡tica en Colab
!pip install ultralytics segment-anything transformers opencv-python matplotlib pillow pandas

# Para uso local
pip install torch torchvision ultralytics segment-anything transformers opencv-python matplotlib pillow pandas
```

### ðŸš€ EjecuciÃ³n RÃ¡pida

1. **Abrir el notebook**: `deteccion_objetos_yolo.ipynb`
2. **Ejecutar configuraciÃ³n**: Primera celda instala dependencias automÃ¡ticamente
3. **Cargar imagen**: Subir imagen de prueba o usar una de ejemplo
4. **Ejecutar pipeline**: Correr todas las celdas secuencialmente
5. **Explorar resultados**: Visualizar detecciones, segmentaciones y efectos

### ðŸŽ›ï¸ ParÃ¡metros Configurables

- **Umbral de confianza YOLO**: Ajustar sensibilidad de detecciÃ³n
- **Intensidad de bokeh**: Controlar nivel de desenfoque
- **TamaÃ±o de pÃ­xeles**: Modificar granularidad del pixelado
- **Clases a detectar**: Filtrar por tipos especÃ­ficos de objetos
- **ResoluciÃ³n de salida**: Ajustar calidad de imÃ¡genes generadas

---

## ðŸ’¡ Casos de Uso

### ðŸ™ï¸ FotografÃ­a Urbana
- DetecciÃ³n de vehÃ­culos y peatones
- Efectos de profundidad en escenas complejas
- AnÃ¡lisis de distribuciÃ³n espacial de objetos

### ðŸ  Interiores y Arquitectura
- SegmentaciÃ³n de muebles y decoraciÃ³n
- AnÃ¡lisis de profundidad espacial
- Efectos artÃ­sticos en espacios

### ðŸŒ¿ Naturaleza y Paisajes
- DetecciÃ³n de animales y objetos naturales
- Efectos de bokeh en fotografÃ­a macro
- AnÃ¡lisis de composiciÃ³n de escenas

### ðŸ“± Redes Sociales y Arte Digital
- Efectos creativos automatizados
- SegmentaciÃ³n para composiciones
- GeneraciÃ³n de contenido visual

---

## ðŸ”¬ AnÃ¡lisis TÃ©cnico

### âš¡ Rendimiento

- **YOLO**: ~50ms por imagen en GPU, ~200ms en CPU
- **SAM**: ~100ms por objeto en GPU, ~500ms en CPU  
- **MiDaS**: ~300ms por imagen en GPU, ~2s en CPU
- **Pipeline completo**: ~1-3 segundos dependiendo del hardware

### ðŸŽ¯ PrecisiÃ³n

- **DetecciÃ³n YOLO**: mAP@0.5 > 0.7 en dataset COCO
- **SegmentaciÃ³n SAM**: IoU > 0.85 con prompts de calidad
- **Profundidad MiDaS**: Error relativo < 15% en escenas tÃ­picas

### ðŸ› ï¸ Limitaciones Identificadas

- **Objetos muy pequeÃ±os**: YOLO puede fallar en detecciÃ³n
- **OclusiÃ³n severa**: SAM tiene dificultades con objetos muy ocluidos
- **Escenas complejas**: MiDaS puede confundirse con reflejos/transparencias
- **Tiempo real**: El pipeline completo no es apto para aplicaciones en vivo

---

## ðŸ’¬ ReflexiÃ³n Final

Este trabajo de grupo nos permitiÃ³ comprender la **integraciÃ³n de mÃºltiples modelos de AI** en un pipeline cohesivo. La combinaciÃ³n de YOLO, SAM y MiDaS demuestra cÃ³mo diferentes especializaciones pueden complementarse para crear aplicaciones mÃ¡s ricas y funcionales.

### ðŸ”‘ Aspectos Clave Aprendidos:

- **Prompting estratÃ©gico**: Usar salidas de un modelo como entradas de otro maximiza la precisiÃ³n
- **GestiÃ³n de memoria**: Los modelos grandes requieren optimizaciÃ³n cuidadosa de recursos
- **ValidaciÃ³n robusta**: Manejo de errores crÃ­tico cuando se integran mÃºltiples sistemas
- **VisualizaciÃ³n efectiva**: La presentaciÃ³n clara de resultados es tan importante como los algoritmos

### ðŸš€ Aplicaciones Futuras:

- **Realidad aumentada**: SegmentaciÃ³n en tiempo real para efectos AR
- **RobÃ³tica**: NavegaciÃ³n y manipulaciÃ³n basada en profundidad y segmentaciÃ³n
- **AnÃ¡lisis mÃ©dico**: SegmentaciÃ³n de estructuras anatÃ³micas con estimaciÃ³n volumÃ©trica
- **AutomociÃ³n**: Sistemas avanzados de asistencia al conductor (ADAS)

### ðŸŽ“ Valor AcadÃ©mico:

Este proyecto integrÃ³ conceptos fundamentales de:
- **Deep Learning**: Arquitecturas CNN, transfer learning, fine-tuning
- **Computer Vision**: DetecciÃ³n, segmentaciÃ³n, estimaciÃ³n de profundidad
- **Procesamiento de ImÃ¡genes**: Filtros, transformaciones, anÃ¡lisis estadÃ­stico
- **IngenierÃ­a de Software**: Pipelines, manejo de errores, modularidad

La implementaciÃ³n prÃ¡ctica demostrÃ³ que la **visiÃ³n computacional moderna** no se trata de un solo algoritmo, sino de la **orquestaciÃ³n inteligente** de mÃºltiples sistemas especializados trabajando en conjunto.
- **Arquitectura virtual**: AplicaciÃ³n precisa de texturas en modelos arquitectÃ³nicos
- **Arte digital**: CreaciÃ³n de efectos visuales mediante manipulaciÃ³n UV
- **Realidad aumentada**: Mapeo preciso de texturas sobre objetos del mundo real
