# üîç Pipeline de Visi√≥n Computacional: Detecci√≥n, Segmentaci√≥n y Estimaci√≥n de Profundidad

üìÖ **Fecha:** 2025-01-07 ‚Äì Fecha de realizaci√≥n

üéØ **Objetivo del Proyecto:**
Desarrollar un pipeline integrado de visi√≥n computacional que combine detecci√≥n de objetos con YOLO, segmentaci√≥n con SAM (Segment Anything Model), y estimaci√≥n de profundidad con MiDaS. El objetivo es crear un sistema completo que detecte objetos, los segmente con precisi√≥n, estime su profundidad, y genere visualizaciones y aplicaciones creativas como efectos de bokeh y an√°lisis de datos.

---

## üß† Conceptos Aprendidos

### üéØ Detecci√≥n de Objetos con YOLO

- **Definici√≥n**: YOLO (You Only Look Once) es un algoritmo de detecci√≥n de objetos en tiempo real que divide la imagen en una cuadr√≠cula y predice bounding boxes y probabilidades de clase
- **Caracter√≠sticas principales**:
  - ‚úÖ Detecci√≥n en una sola pasada de la red neuronal
  - ‚úÖ Predicci√≥n simult√°nea de m√∫ltiples objetos
  - ‚úÖ Salida con coordenadas de bounding boxes y confianza
  - ‚úÖ Soporte para m√∫ltiples clases de objetos (80+ en COCO dataset)

### ‚úÇÔ∏è Segmentaci√≥n con SAM (Segment Anything Model)

- **Prompt-based Segmentation**: SAM utiliza prompts (puntos, boxes, texto) para generar m√°scaras de segmentaci√≥n precisas
- **Box Prompting**: Uso de bounding boxes de YOLO como prompts para SAM
- **Mask Generation**: Producci√≥n de m√°scaras binarias de alta calidad
- **Zero-shot Capability**: Capacidad de segmentar objetos sin entrenamiento espec√≠fico

### üìè Estimaci√≥n de Profundidad con MiDaS

- **Monocular Depth Estimation**: Estimaci√≥n de profundidad a partir de una sola imagen
- **Relative Depth**: MiDaS produce mapas de profundidad relativa (no absoluta)
- **Multi-scale Processing**: Procesamiento a diferentes escalas para mejor precisi√≥n
- **Dense Prediction**: Cada p√≠xel recibe una estimaci√≥n de profundidad

### üé® Aplicaciones Creativas y Anal√≠ticas

- **Efecto Bokeh**: Desenfoque selectivo basado en profundidad
- **Pixelado de Fondo**: Aplicaci√≥n de efectos a regiones no segmentadas
- **An√°lisis Estad√≠stico**: Extracci√≥n de m√©tricas de √°rea, profundidad y distribuci√≥n
- **Exportaci√≥n de Datos**: Generaci√≥n de reportes en formato CSV

---

## üîß Herramientas y Entornos

- **Python 3.8+** - Lenguaje de programaci√≥n principal
- **PyTorch** - Framework de deep learning para YOLO y SAM
- **OpenCV** - Procesamiento de im√°genes y visualizaci√≥n
- **Transformers** - Modelos pre-entrenados de Hugging Face
- **NumPy** - Operaciones num√©ricas y manipulaci√≥n de arrays
- **Matplotlib** - Visualizaci√≥n de resultados y gr√°ficos
- **Pillow (PIL)** - Procesamiento adicional de im√°genes
- **Pandas** - An√°lisis y exportaci√≥n de datos
- **Google Colab** - Entorno de desarrollo en la nube con GPU
---

## üìÅ Estructura del Proyecto

```
Pipeline_Vision_Computacional/
‚îú‚îÄ‚îÄ deteccion_objetos_yolo.ipynb     # Notebook principal del pipeline
‚îú‚îÄ‚îÄ README.md                        # Esta documentaci√≥n
‚îú‚îÄ‚îÄ informeParte1.md  
‚îî‚îÄ‚îÄ resultados/                      # Im√°genes generadas y an√°lisis
```

---

## üß™ Implementaci√≥n

### üîπ Etapas realizadas

1. **Configuraci√≥n del entorno**: Instalaci√≥n autom√°tica de dependencias para Colab y local
2. **Detecci√≥n con YOLO**: Implementaci√≥n de YOLOv8 para detectar m√∫ltiples clases de objetos
3. **Segmentaci√≥n con SAM**: Uso de bounding boxes como prompts para generar m√°scaras precisas
4. **Estimaci√≥n de profundidad**: Integraci√≥n de MiDaS para mapas de profundidad densos
5. **Visualizaci√≥n combinada**: Overlay de detecciones, m√°scaras y profundidad
6. **Aplicaciones creativas**: Efectos de bokeh y pixelado basados en segmentaci√≥n
7. **An√°lisis de datos**: Extracci√≥n de m√©tricas y exportaci√≥n a CSV

### üîπ Funciones principales implementadas

**Detecci√≥n de objetos con YOLO:**

```python
def detectar_objetos_yolo(imagen_path, modelo_yolo, umbral_confianza=0.5):
    """
    Detecta objetos en una imagen usando YOLO.
    
    Args:
        imagen_path: Ruta a la imagen
        modelo_yolo: Modelo YOLO pre-entrenado
        umbral_confianza: Umbral m√≠nimo de confianza
    
    Returns:
        tuple: (imagen_original, detecciones, boxes, clases, confianzas)
    """
    imagen = cv2.imread(imagen_path)
    imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)
    
    resultados = modelo_yolo(imagen_rgb, conf=umbral_confianza)
    
    # Extracci√≥n de bounding boxes, clases y confianzas
    for resultado in resultados:
        boxes = resultado.boxes.xyxy.cpu().numpy()
        clases = resultado.boxes.cls.cpu().numpy()
        confianzas = resultado.boxes.conf.cpu().numpy()
    
    return imagen_rgb, resultados, boxes, clases, confianzas
```

**Segmentaci√≥n con SAM usando prompts de YOLO:**

```python
def segmentar_con_sam(imagen, boxes, predictor_sam):
    """
    Realiza segmentaci√≥n usando SAM con bounding boxes como prompts.
    
    Args:
        imagen: Imagen de entrada (numpy array)
        boxes: Bounding boxes de YOLO
        predictor_sam: Predictor de SAM
    
    Returns:
        list: Lista de m√°scaras de segmentaci√≥n
    """
    predictor_sam.set_image(imagen)
    mascaras = []
    
    for box in boxes:
        # Convertir box a formato SAM (xyxy)
        input_box = np.array(box).astype(np.int32)
        
        # Generar m√°scara usando el box como prompt
        masks, scores, logits = predictor_sam.predict(
            point_coords=None,
            point_labels=None,
            box=input_box[None, :],
            multimask_output=False,
        )
        
        mascaras.append(masks[0])
    
    return mascaras
```

**Estimaci√≥n de profundidad con MiDaS:**

```python
def estimar_profundidad_midas(imagen, modelo_midas, transform_midas, device):
    """
    Estima la profundidad de una imagen usando MiDaS.
    
    Args:
        imagen: Imagen de entrada
        modelo_midas: Modelo MiDaS pre-entrenado
        transform_midas: Transformaciones para MiDaS
        device: Dispositivo (CPU/GPU)
    
    Returns:
        numpy.ndarray: Mapa de profundidad normalizado
    """
    # Preprocesar imagen para MiDaS
    input_batch = transform_midas(imagen).to(device)
    
    with torch.no_grad():
        prediction = modelo_midas(input_batch)
        
        # Interpolar a tama√±o original
        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=imagen.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze()
    
    # Normalizar a rango [0, 1]
    depth_map = prediction.cpu().numpy()
    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
    
    return depth_map
```

**Efecto bokeh basado en profundidad:**

```python
def efecto_bokeh_profundidad(imagen, mapa_profundidad, mascaras, intensidad_blur=15):
    """
    Aplica efecto bokeh basado en profundidad y segmentaci√≥n.
    
    Args:
        imagen: Imagen original
        mapa_profundidad: Mapa de profundidad
        mascaras: M√°scaras de segmentaci√≥n
        intensidad_blur: Intensidad del desenfoque
    
    Returns:
        numpy.ndarray: Imagen con efecto bokeh
    """
    imagen_bokeh = imagen.copy()
    
    # Crear m√°scara combinada de objetos detectados
    mascara_objetos = np.zeros(imagen.shape[:2], dtype=bool)
    for mascara in mascaras:
        mascara_objetos |= mascara
    
    # Aplicar blur variable basado en profundidad
    imagen_blur = cv2.GaussianBlur(imagen, (intensidad_blur, intensidad_blur), 0)
    
    # Mezclar imagen original y blur basado en profundidad y segmentaci√≥n
    for y in range(imagen.shape[0]):
        for x in range(imagen.shape[1]):
            if not mascara_objetos[y, x]:  # Fondo
                factor_blur = mapa_profundidad[y, x]
                alpha = np.clip(factor_blur * 1.5, 0, 1)
                imagen_bokeh[y, x] = (1 - alpha) * imagen[y, x] + alpha * imagen_blur[y, x]
    
    return imagen_bokeh.astype(np.uint8)
```

---

## üìä Resultados y An√°lisis

### üìå Imagen Original
![Pipeline de Visi√≥n Computacional](./resultados/Origin.png)
### üìå Pruebas YOLO

![Pipeline de Visi√≥n Computacional](./resultados/DetallesYolo.png)
![Pipeline de Visi√≥n Computacional](./resultados/DeteccionYolo.png)
![Pipeline de Visi√≥n Computacional](./resultados/DistribucionYolo.png)

- ‚úÖ **Precisi√≥n**: Detecci√≥n exitosa de m√∫ltiples clases (personas, veh√≠culos, objetos)
- ‚úÖ **Bounding Boxes**: Coordenadas precisas 

#### Multiples Objetos
![Pipeline de Visi√≥n Computacional](./resultados/Cocina.png)


### üìå Pipeline Completo en Acci√≥n:

![Pipeline de Visi√≥n Computacional](./resultados/DiagramaPipeline.png)

#### RESULTADO YOLO SAM MiDas
![Pipeline de Visi√≥n Computacional](./resultados/DiagramaPipeline.png)


![Pipeline de Visi√≥n Computacional](./resultados/YoloSam.png)
### üéØ Detecciones YOLO:
- ‚úÖ **Precisi√≥n**: Detecci√≥n exitosa de m√∫ltiples clases (personas, veh√≠culos, objetos)
- ‚úÖ **Bounding Boxes**: Coordenadas precisas para usar como prompts SAM
- ‚úÖ **Confianza**: Filtrado por umbral de confianza configurable
- ‚úÖ **Clases**: Soporte para 80+ clases del dataset COCO

![Pipeline de Visi√≥n Computacional](./resultados/Galeria.png)
### ‚úÇÔ∏è Segmentaci√≥n SAM:

- ‚úÖ **M√°scaras precisas**: Segmentaci√≥n pixel-perfect usando prompts de YOLO
- ‚úÖ **M√∫ltiples objetos**: Procesamiento simult√°neo de varios objetos detectados
- ‚úÖ **Alta calidad**: Bordes suaves y precisos en las m√°scaras
- ‚úÖ **Robustez**: Funciona bien con objetos parcialmente ocluidos


![Pipeline de Visi√≥n Computacional](./resultados/Profundidad.png)
### üìè Mapas de Profundidad:

- ‚úÖ **Profundidad relativa**: Estimaci√≥n consistente de distancias relativas
- ‚úÖ **Resoluci√≥n completa**: Mapas densos para cada p√≠xel
- ‚úÖ **Coherencia espacial**: Transiciones suaves entre regiones
- ‚úÖ **Compatibilidad**: Funciona con im√°genes de cualquier resoluci√≥n


![Pipeline de Visi√≥n Computacional](./resultados/EfectosVisuales.png)
### üé® Aplicaciones Creativas:

- ‚úÖ **Efecto Bokeh**: Desenfoque realista basado en profundidad
- ‚úÖ **Pixelado selectivo**: Efectos art√≠sticos en fondo/objetos
- ‚úÖ **Recortes precisos**: Extracci√≥n de objetos segmentados
- ‚úÖ **Galer√≠a interactiva**: Visualizaci√≥n organizada de resultados



![Pipeline de Visi√≥n Computacional](./resultados/Clasificacion.png)
### üìà An√°lisis Estad√≠stico:

- ‚úÖ **M√©tricas de √°rea**: C√°lculo de √°reas de objetos segmentados
- ‚úÖ **An√°lisis de profundidad**: Estad√≠sticas de distribuci√≥n de profundidad
- ‚úÖ **Exportaci√≥n CSV**: Datos estructurados para an√°lisis posterior
- ‚úÖ **Visualizaciones**: Gr√°ficos de distribuci√≥n y comparativas

---

## üîß Configuraci√≥n y Uso

### üêç Requisitos del Sistema

```bash
# Instalaci√≥n autom√°tica en Colab
!pip install ultralytics segment-anything transformers opencv-python matplotlib pillow pandas

# Para uso local
pip install torch torchvision ultralytics segment-anything transformers opencv-python matplotlib pillow pandas
```

### üöÄ Ejecuci√≥n R√°pida

1. **Abrir el notebook**: `deteccion_objetos_yolo.ipynb`
2. **Ejecutar configuraci√≥n**: Primera celda instala dependencias autom√°ticamente
3. **Cargar imagen**: Subir imagen de prueba o usar una de ejemplo
4. **Ejecutar pipeline**: Correr todas las celdas secuencialmente
5. **Explorar resultados**: Visualizar detecciones, segmentaciones y efectos

### üéõÔ∏è Par√°metros Configurables

- **Umbral de confianza YOLO**: Ajustar sensibilidad de detecci√≥n
- **Intensidad de bokeh**: Controlar nivel de desenfoque
- **Tama√±o de p√≠xeles**: Modificar granularidad del pixelado
- **Clases a detectar**: Filtrar por tipos espec√≠ficos de objetos
- **Resoluci√≥n de salida**: Ajustar calidad de im√°genes generadas

---

## üí° Casos de Uso

### üèôÔ∏è Fotograf√≠a Urbana
- Detecci√≥n de veh√≠culos y peatones
- Efectos de profundidad en escenas complejas
- An√°lisis de distribuci√≥n espacial de objetos

### üè† Interiores y Arquitectura
- Segmentaci√≥n de muebles y decoraci√≥n
- An√°lisis de profundidad espacial
- Efectos art√≠sticos en espacios

### üåø Naturaleza y Paisajes
- Detecci√≥n de animales y objetos naturales
- Efectos de bokeh en fotograf√≠a macro
- An√°lisis de composici√≥n de escenas

### üì± Redes Sociales y Arte Digital
- Efectos creativos automatizados
- Segmentaci√≥n para composiciones
- Generaci√≥n de contenido visual

---

## üî¨ An√°lisis T√©cnico

### ‚ö° Rendimiento

- **YOLO**: ~50ms por imagen en GPU, ~200ms en CPU
- **SAM**: ~100ms por objeto en GPU, ~500ms en CPU  
- **MiDaS**: ~300ms por imagen en GPU, ~2s en CPU
- **Pipeline completo**: ~1-3 segundos dependiendo del hardware

### üéØ Precisi√≥n

- **Detecci√≥n YOLO**: mAP@0.5 > 0.7 en dataset COCO
- **Segmentaci√≥n SAM**: IoU > 0.85 con prompts de calidad
- **Profundidad MiDaS**: Error relativo < 15% en escenas t√≠picas

### üõ†Ô∏è Limitaciones Identificadas

- **Objetos muy peque√±os**: YOLO puede fallar en detecci√≥n
- **Oclusi√≥n severa**: SAM tiene dificultades con objetos muy ocluidos
- **Escenas complejas**: MiDaS puede confundirse con reflejos/transparencias
- **Tiempo real**: El pipeline completo no es apto para aplicaciones en vivo

---

## üí¨ Reflexi√≥n Final

Este trabajo de grupo nos permiti√≥ comprender la **integraci√≥n de m√∫ltiples modelos de AI** en un pipeline cohesivo. La combinaci√≥n de YOLO, SAM y MiDaS demuestra c√≥mo diferentes especializaciones pueden complementarse para crear aplicaciones m√°s ricas y funcionales.

### üîë Aspectos Clave Aprendidos:

- **Prompting estrat√©gico**: Usar salidas de un modelo como entradas de otro maximiza la precisi√≥n
- **Gesti√≥n de memoria**: Los modelos grandes requieren optimizaci√≥n cuidadosa de recursos
- **Validaci√≥n robusta**: Manejo de errores cr√≠tico cuando se integran m√∫ltiples sistemas
- **Visualizaci√≥n efectiva**: La presentaci√≥n clara de resultados es tan importante como los algoritmos

### üöÄ Aplicaciones Futuras:

- **Realidad aumentada**: Segmentaci√≥n en tiempo real para efectos AR
- **Rob√≥tica**: Navegaci√≥n y manipulaci√≥n basada en profundidad y segmentaci√≥n
- **An√°lisis m√©dico**: Segmentaci√≥n de estructuras anat√≥micas con estimaci√≥n volum√©trica
- **Automoci√≥n**: Sistemas avanzados de asistencia al conductor (ADAS)

### üéì Valor Acad√©mico:

Este proyecto integr√≥ conceptos fundamentales de:
- **Deep Learning**: Arquitecturas CNN, transfer learning, fine-tuning
- **Computer Vision**: Detecci√≥n, segmentaci√≥n, estimaci√≥n de profundidad
- **Procesamiento de Im√°genes**: Filtros, transformaciones, an√°lisis estad√≠stico
- **Ingenier√≠a de Software**: Pipelines, manejo de errores, modularidad

La implementaci√≥n pr√°ctica demostr√≥ que la **visi√≥n computacional moderna** no se trata de un solo algoritmo, sino de la **orquestaci√≥n inteligente** de m√∫ltiples sistemas especializados trabajando en conjunto.
- **Arquitectura virtual**: Aplicaci√≥n precisa de texturas en modelos arquitect√≥nicos
- **Arte digital**: Creaci√≥n de efectos visuales mediante manipulaci√≥n UV
- **Realidad aumentada**: Mapeo preciso de texturas sobre objetos del mundo real
