#!/usr/bin/env python3
"""
ğŸ§ª Taller - Interfaces Multimodales: Uniendo Voz y Gestos
Sistema que combina detecciÃ³n de gestos (MediaPipe) con reconocimiento de voz
para crear una experiencia de interacciÃ³n multimodal rica e intuitiva.
"""

import cv2
import mediapipe as mp
import speech_recognition as sr
import pygame
import numpy as np
import threading
import time
import math
import queue
from typing import Optional, Tuple, Dict, Any
import pyttsx3

# ConfiguraciÃ³n de colores
COLORS = {
    'azul': (100, 150, 255),
    'rojo': (255, 100, 100),
    'verde': (100, 255, 100),
    'amarillo': (255, 255, 100),
    'blanco': (255, 255, 255),
    'negro': (0, 0, 0),
    'gris': (128, 128, 128),
    'violeta': (255, 100, 255)
}

# Comandos de voz reconocidos
VOICE_COMMANDS = [
    'cambiar', 'mover', 'rotar', 'mostrar', 'salir',
    'azul', 'rojo', 'verde', 'amarillo', 'parar', 'reset'
]

class GestureDetector:
    """Detector de gestos usando MediaPipe Hands"""
    
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        
    def classify_gesture(self, hand_landmarks) -> str:
        """Clasifica el gesto basado en las posiciones de los landmarks"""
        if not hand_landmarks:
            return "none"
        
        landmarks = hand_landmarks.landmark
        
        # Obtener posiciones de los dedos
        thumb_tip = landmarks[4]
        index_tip = landmarks[8]
        middle_tip = landmarks[12]
        ring_tip = landmarks[16]
        pinky_tip = landmarks[20]
        
        # Posiciones de las articulaciones medias
        index_pip = landmarks[6]
        middle_pip = landmarks[10]
        ring_pip = landmarks[14]
        pinky_pip = landmarks[18]
        
        # Contar dedos extendidos
        fingers_up = []
        
        # Pulgar (comparar x para mano derecha/izquierda)
        if thumb_tip.x > landmarks[3].x:  # Mano derecha
            fingers_up.append(thumb_tip.x > landmarks[3].x)
        else:  # Mano izquierda
            fingers_up.append(thumb_tip.x < landmarks[3].x)
            
        # Otros dedos (comparar y)
        fingers_up.append(index_tip.y < index_pip.y)
        fingers_up.append(middle_tip.y < middle_pip.y)
        fingers_up.append(ring_tip.y < ring_pip.y)
        fingers_up.append(pinky_tip.y < pinky_pip.y)
        
        total_fingers = sum(fingers_up)
        
        # Clasificar gestos
        if total_fingers == 0:
            return "fist"  # PuÃ±o cerrado
        elif total_fingers == 5:
            return "open"  # Mano abierta
        elif total_fingers == 2 and fingers_up[1] and fingers_up[2]:
            return "peace"  # Dos dedos (V de victoria)
        elif total_fingers == 1 and fingers_up[1]:
            return "point"  # SeÃ±alar
        elif self._is_ok_gesture(landmarks):
            return "ok"  # Gesto OK
        else:
            return f"fingers_{total_fingers}"
    
    def _is_ok_gesture(self, landmarks) -> bool:
        """Detecta el gesto OK (cÃ­rculo con pulgar e Ã­ndice)"""
        thumb_tip = landmarks[4]
        index_tip = landmarks[8]
        
        # Calcular distancia entre pulgar e Ã­ndice
        distance = math.sqrt(
            (thumb_tip.x - index_tip.x) ** 2 + 
            (thumb_tip.y - index_tip.y) ** 2
        )
        
        return distance < 0.05  # Umbral para considerar que estÃ¡n tocÃ¡ndose
    
    def detect_gestures(self, frame):
        """Detecta gestos en el frame y retorna informaciÃ³n"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)
        
        gestures = []
        annotated_frame = frame.copy()
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # Dibujar landmarks
                self.mp_drawing.draw_landmarks(
                    annotated_frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS
                )
                
                # Clasificar gesto
                gesture = self.classify_gesture(hand_landmarks)
                gestures.append(gesture)
                
                # Obtener posiciÃ³n de la muÃ±eca para tracking
                wrist = hand_landmarks.landmark[0]
                hand_center = (int(wrist.x * frame.shape[1]), int(wrist.y * frame.shape[0]))
                
                # Dibujar etiqueta del gesto
                cv2.putText(annotated_frame, gesture, 
                           (hand_center[0] - 50, hand_center[1] - 50),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        return gestures, annotated_frame

class VoiceRecognizer:
    """Reconocedor de voz usando SpeechRecognition"""
    
    def __init__(self, microphone_index=None):
        self.recognizer = sr.Recognizer()
        self.microphone_index = microphone_index
        self.is_listening = False
        self.command_queue = queue.Queue()
        
        # Configurar micrÃ³fono
        self.setup_microphone()
        
        # Ajustar configuraciÃ³n para EMEET SmartCam
        self.optimize_for_emeet()
        
        # Sistema de sÃ­ntesis de voz
        self.tts_engine = pyttsx3.init()
        self.tts_engine.setProperty('rate', 150)  # Velocidad de habla
    
    def setup_microphone(self):
        """Configura el micrÃ³fono con mejor detecciÃ³n"""
        try:
            # Intentar con micrÃ³fono especÃ­fico si se proporcionÃ³
            if self.microphone_index is not None:
                print(f"ğŸ¤ Usando micrÃ³fono especÃ­fico #{self.microphone_index}")
                self.microphone = sr.Microphone(device_index=self.microphone_index)
            else:
                # Buscar el mejor micrÃ³fono disponible
                self.microphone = self.find_best_microphone()
            
            # Configurar el reconocedor
            print("ğŸ”§ Configurando micrÃ³fono...")
            with self.microphone as source:
                self.recognizer.adjust_for_ambient_noise(source, duration=2)
                print(f"âœ… MicrÃ³fono configurado - Umbral: {self.recognizer.energy_threshold}")
                
        except Exception as e:
            print(f"âš ï¸  Error configurando micrÃ³fono: {e}")
            print("ğŸ”„ Intentando con micrÃ³fono por defecto...")
            try:
                self.microphone = sr.Microphone()
                with self.microphone as source:
                    self.recognizer.adjust_for_ambient_noise(source, duration=1)
            except Exception as e2:
                print(f"âŒ Error crÃ­tico con micrÃ³fono: {e2}")
                raise e2
    
    def find_best_microphone(self):
        """Encuentra el mejor micrÃ³fono disponible, priorizando EMEET SmartCam Nova 4K"""
        import pyaudio
        
        try:
            p = pyaudio.PyAudio()
            
            # Buscar especÃ­ficamente EMEET SmartCam Nova 4K
            print("ğŸ” Buscando micrÃ³fono EMEET SmartCam Nova 4K...")
            for i in range(p.get_device_count()):
                try:
                    info = p.get_device_info_by_index(i)
                    if info['maxInputChannels'] > 0:
                        # Buscar EMEET SmartCam en el nombre
                        if "EMEET" in info['name'] or "SmartCam" in info['name']:
                            print(f"ğŸ¯ Â¡Encontrado! Usando: {info['name']}")
                            test_mic = sr.Microphone(device_index=i)
                            p.terminate()
                            return test_mic
                except:
                    continue
            
            print("âš ï¸  No se encontrÃ³ EMEET SmartCam, buscando otros micrÃ³fonos...")
            
            # Obtener micrÃ³fono por defecto como respaldo
            try:
                default_info = p.get_default_input_device_info()
                default_mic = sr.Microphone(device_index=default_info['index'])
                print(f"ğŸ¤ Usando micrÃ³fono por defecto: {default_info['name']}")
                p.terminate()
                return default_mic
            except:
                pass
            
            # Buscar cualquier micrÃ³fono funcional
            for i in range(p.get_device_count()):
                try:
                    info = p.get_device_info_by_index(i)
                    if info['maxInputChannels'] > 0:
                        # Intentar crear micrÃ³fono
                        test_mic = sr.Microphone(device_index=i)
                        print(f"ğŸ¤ Usando micrÃ³fono: {info['name']}")
                        p.terminate()
                        return test_mic
                except:
                    continue
            
                        # p.terminate()
            # Si nada funciona, usar micrÃ³fono por defecto sin Ã­ndice
            return sr.Microphone()
            
        except Exception as e:
            print(f"âš ï¸  Error buscando micrÃ³fonos: {e}")
            return sr.Microphone()
    
    def optimize_for_emeet(self):
        """Optimiza la configuraciÃ³n especÃ­ficamente para EMEET SmartCam Nova 4K"""
        try:
            # Ajustar configuraciÃ³n especÃ­fica para EMEET SmartCam
            # Estos valores funcionan mejor con cÃ¡maras web
            self.recognizer.energy_threshold = 300  # Umbral mÃ¡s bajo para cÃ¡maras web
            self.recognizer.dynamic_energy_threshold = True
            self.recognizer.dynamic_energy_adjustment_damping = 0.15
            self.recognizer.dynamic_energy_ratio = 1.5
            self.recognizer.pause_threshold = 0.8  # Pausa mÃ¡s corta
            self.recognizer.operation_timeout = None
            self.recognizer.phrase_threshold = 0.3
            self.recognizer.non_speaking_duration = 0.5
            
            print("ğŸ”§ ConfiguraciÃ³n optimizada para EMEET SmartCam Nova 4K")
            
        except Exception as e:
            print(f"âš ï¸  Error optimizando para EMEET: {e}")
         
    def listen_continuously(self):
        """Escucha continuamente comandos de voz en un hilo separado"""
        self.is_listening = True
        
        def listen_worker():
            while self.is_listening:
                try:
                    with self.microphone as source:
                        # Escuchar con timeout corto para no bloquear
                        audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=3)
                    
                    try:
                        # Reconocer el audio
                        command = self.recognizer.recognize_google(audio, language='es-ES').lower()
                        
                        # Filtrar solo comandos vÃ¡lidos
                        for valid_command in VOICE_COMMANDS:
                            if valid_command in command:
                                self.command_queue.put(valid_command)
                                print(f"ğŸ¤ Comando reconocido: {valid_command}")
                                break
                                
                    except sr.UnknownValueError:
                        pass  # No se entendiÃ³ el audio
                    except sr.RequestError as e:
                        print(f"Error en el servicio de reconocimiento: {e}")
                        
                except sr.WaitTimeoutError:
                    pass  # Timeout normal, continuar
                except Exception as e:
                    print(f"Error en reconocimiento de voz: {e}")
                    time.sleep(0.1)
        
        # Iniciar hilo de escucha
        self.listen_thread = threading.Thread(target=listen_worker, daemon=True)
        self.listen_thread.start()
    
    def get_command(self) -> Optional[str]:
        """Obtiene el siguiente comando de la cola"""
        try:
            return self.command_queue.get_nowait()
        except queue.Empty:
            return None
    
    def speak(self, text: str):
        """SÃ­ntesis de voz para retroalimentaciÃ³n"""
        def speak_worker():
            self.tts_engine.say(text)
            self.tts_engine.runAndWait()
        
        thread = threading.Thread(target=speak_worker, daemon=True)
        thread.start()
    
    def stop_listening(self):
        """Detiene la escucha continua"""
        self.is_listening = False

class InteractiveObject:
    """Objeto interactivo que responde a comandos multimodales"""
    
    def __init__(self, x: int, y: int, size: int = 50):
        self.x = x
        self.y = y
        self.size = size
        self.color = COLORS['azul']
        self.rotation = 0
        self.scale = 1.0
        self.velocity = [0, 0]
        self.is_moving = False
        self.is_rotating = False
        
    def update(self):
        """Actualiza el estado del objeto"""
        if self.is_moving:
            self.x += self.velocity[0]
            self.y += self.velocity[1]
            
        if self.is_rotating:
            self.rotation += 2
            
        # Mantener dentro de los lÃ­mites
        self.x = max(self.size, min(800 - self.size, self.x))
        self.y = max(self.size, min(600 - self.size, self.y))
        
    def draw(self, screen):
        """Dibuja el objeto en la pantalla"""
        # Crear superficie para rotaciÃ³n
        surf_size = int(self.size * 2 * self.scale)
        surf = pygame.Surface((surf_size, surf_size), pygame.SRCALPHA)
        
        # Dibujar forma bÃ¡sica
        pygame.draw.circle(surf, self.color, 
                          (surf_size // 2, surf_size // 2), 
                          int(self.size * self.scale))
        
        # Rotar superficie
        if self.rotation != 0:
            surf = pygame.transform.rotate(surf, self.rotation)
        
        # Dibujar en pantalla
        rect = surf.get_rect(center=(self.x, self.y))
        screen.blit(surf, rect)

class MultimodalInterface:
    """Interfaz principal que coordina gestos, voz y visualizaciÃ³n"""
    
    def __init__(self):
        # Inicializar Pygame
        pygame.init()
        self.screen = pygame.display.set_mode((800, 600))
        pygame.display.set_caption("ğŸ§ª Interfaces Multimodales: Voz + Gestos")
        self.clock = pygame.time.Clock()
        self.font = pygame.font.Font(None, 24)
        self.big_font = pygame.font.Font(None, 36)
        
        # Inicializar componentes
        self.gesture_detector = GestureDetector()
        self.voice_recognizer = VoiceRecognizer()
        
        # Estado del sistema
        self.running = True
        self.current_gesture = "none"
        self.last_command = ""
        self.interactive_object = InteractiveObject(400, 300, 40)
        
        # EstadÃ­sticas y feedback
        self.gesture_count = 0
        self.command_count = 0
        self.multimodal_actions = 0
        self.status_message = "ğŸ¯ Combina gestos y voz para interactuar"
        self.message_timer = 0
        
        # Inicializar cÃ¡mara
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            raise Exception("No se pudo abrir la cÃ¡mara")
            
        # Configurar ventana de cÃ¡mara
        cv2.namedWindow('DetecciÃ³n de Gestos', cv2.WINDOW_NORMAL)
        cv2.resizeWindow('DetecciÃ³n de Gestos', 640, 480)
        
    def start(self):
        """Inicia el sistema multimodal"""
        print("ğŸš€ Iniciando sistema multimodal...")
        print("ğŸ“ Comandos disponibles:")
        print("   Gestos: mano abierta, puÃ±o, dos dedos, gesto OK")
        print("   Voz: cambiar, mover, rotar, mostrar, colores, salir")
        
        # Iniciar reconocimiento de voz
        self.voice_recognizer.listen_continuously()
        self.voice_recognizer.speak("Sistema multimodal iniciado")
        
        # Bucle principal
        while self.running:
            self.handle_events()
            self.process_camera()
            self.process_voice()
            self.update_objects()
            self.render()
            self.clock.tick(60)
        
        self.cleanup()
    
    def handle_events(self):
        """Maneja eventos de Pygame"""
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                self.running = False
            elif event.type == pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE:
                    self.running = False
                elif event.key == pygame.K_r:
                    self.reset_object()
    
    def process_camera(self):
        """Procesa el feed de la cÃ¡mara y detecta gestos"""
        ret, frame = self.cap.read()
        if not ret:
            return
        
        # Voltear frame horizontalmente para efecto espejo
        frame = cv2.flip(frame, 1)
        
        # Detectar gestos
        gestures, annotated_frame = self.gesture_detector.detect_gestures(frame)
        
        if gestures:
            new_gesture = gestures[0]  # Usar el primer gesto detectado
            if new_gesture != self.current_gesture:
                self.current_gesture = new_gesture
                self.gesture_count += 1
                print(f"ğŸ‘‹ Gesto detectado: {new_gesture}")
        else:
            self.current_gesture = "none"
        
        # Mostrar frame con anotaciones
        cv2.putText(annotated_frame, f"Gesto: {self.current_gesture}", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(annotated_frame, "ESC: Salir | R: Reset", 
                   (10, annotated_frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        cv2.imshow('DetecciÃ³n de Gestos', annotated_frame)
        cv2.waitKey(1)
    
    def process_voice(self):
        """Procesa comandos de voz"""
        command = self.voice_recognizer.get_command()
        if command:
            self.last_command = command
            self.command_count += 1
            self.process_multimodal_action(self.current_gesture, command)
    
    def process_multimodal_action(self, gesture: str, command: str):
        """Procesa acciones que requieren combinaciÃ³n de gesto y voz"""
        action_performed = False
        message = ""
        
        # Acciones que requieren combinaciones especÃ­ficas
        if command == "cambiar" and gesture == "open":
            self.change_object_color()
            message = "ğŸ¨ Color cambiado con mano abierta"
            action_performed = True
            
        elif command == "mover" and gesture == "peace":
            self.toggle_object_movement()
            message = "ğŸƒ Movimiento activado con dos dedos"
            action_performed = True
            
        elif command == "rotar" and gesture == "fist":
            self.toggle_object_rotation()
            message = "ğŸŒ€ RotaciÃ³n activada con puÃ±o"
            action_performed = True
            
        elif command == "mostrar" and gesture == "ok":
            self.show_statistics()
            message = "ğŸ“Š EstadÃ­sticas mostradas con gesto OK"
            action_performed = True
            
        # Comandos de color (solo con mano abierta)
        elif command in COLORS and gesture == "open":
            self.interactive_object.color = COLORS[command]
            message = f"ğŸ¨ Color cambiado a {command}"
            action_performed = True
            
        # Comandos especiales
        elif command == "salir":
            self.running = False
            message = "ğŸ‘‹ Cerrando aplicaciÃ³n"
            action_performed = True
            
        elif command == "reset":
            self.reset_object()
            message = "ğŸ”„ Objeto reiniciado"
            action_performed = True
            
        elif command == "parar":
            self.stop_all_actions()
            message = "â¹ï¸ Todas las acciones detenidas"
            action_performed = True
        
        # Acciones que no requieren gesto especÃ­fico
        elif command == "mostrar":
            self.show_statistics()
            message = "ğŸ“Š EstadÃ­sticas mostradas"
            action_performed = True
            
        elif command == "mover":
            self.toggle_object_movement()
            message = "ğŸƒ Movimiento activado/desactivado por voz"
            action_performed = True
            
        elif command == "rotar":
            self.toggle_object_rotation()
            message = "ğŸŒ€ RotaciÃ³n activada/desactivada por voz"
            action_performed = True
        
        # Si no se realizÃ³ ninguna acciÃ³n vÃ¡lida
        if not action_performed:
            message = f"âŒ CombinaciÃ³n no vÃ¡lida: {gesture} + {command}"
            self.voice_recognizer.speak("CombinaciÃ³n no vÃ¡lida")
        else:
            self.multimodal_actions += 1
            self.voice_recognizer.speak("AcciÃ³n realizada")
        
        self.set_status_message(message)
        print(f"ğŸ¯ {message}")
    
    def change_object_color(self):
        """Cambia el color del objeto aleatoriamente"""
        colors = list(COLORS.values())
        current_color = self.interactive_object.color
        new_color = current_color
        while new_color == current_color:
            new_color = colors[np.random.randint(len(colors))]
        self.interactive_object.color = new_color
    
    def toggle_object_movement(self):
        """Activa/desactiva el movimiento del objeto"""
        self.interactive_object.is_moving = not self.interactive_object.is_moving
        if self.interactive_object.is_moving:
            # Generar velocidad aleatoria
            angle = np.random.random() * 2 * np.pi
            speed = 2
            self.interactive_object.velocity = [
                speed * np.cos(angle),
                speed * np.sin(angle)
            ]
        else:
            self.interactive_object.velocity = [0, 0]
    
    def toggle_object_rotation(self):
        """Activa/desactiva la rotaciÃ³n del objeto"""
        self.interactive_object.is_rotating = not self.interactive_object.is_rotating
    
    def stop_all_actions(self):
        """Detiene todas las acciones del objeto"""
        self.interactive_object.is_moving = False
        self.interactive_object.is_rotating = False
        self.interactive_object.velocity = [0, 0]
    
    def reset_object(self):
        """Reinicia el objeto a su estado inicial"""
        self.interactive_object = InteractiveObject(400, 300, 40)
        self.multimodal_actions += 1
    
    def show_statistics(self):
        """Muestra estadÃ­sticas del sistema"""
        stats = f"Gestos: {self.gesture_count}, Comandos: {self.command_count}, Acciones: {self.multimodal_actions}"
        print(f"ğŸ“Š EstadÃ­sticas: {stats}")
        self.set_status_message(f"ğŸ“Š {stats}")
    
    def set_status_message(self, message: str):
        """Establece un mensaje de estado temporal"""
        self.status_message = message
        self.message_timer = pygame.time.get_ticks() + 3000  # 3 segundos
    
    def update_objects(self):
        """Actualiza todos los objetos interactivos"""
        self.interactive_object.update()
        
        # Limpiar mensaje de estado despuÃ©s del tiempo
        if pygame.time.get_ticks() > self.message_timer:
            self.status_message = "ğŸ¯ Combina gestos y voz para interactuar"
    
    def render(self):
        """Renderiza toda la interfaz"""
        # Limpiar pantalla
        self.screen.fill(COLORS['negro'])
        
        # Dibujar objeto interactivo
        self.interactive_object.draw(self.screen)
        
        # Panel de informaciÃ³n
        self.draw_info_panel()
        
        # Actualizar pantalla
        pygame.display.flip()
    
    def draw_info_panel(self):
        """Dibuja el panel de informaciÃ³n"""
        y_offset = 10
        
        # TÃ­tulo
        title = self.big_font.render("ğŸ§ª Interfaces Multimodales", True, COLORS['blanco'])
        self.screen.blit(title, (10, y_offset))
        y_offset += 40
        
        # Estado actual
        info_texts = [
            f"ğŸ‘‹ Gesto actual: {self.current_gesture}",
            f"ğŸ¤ Ãšltimo comando: {self.last_command}",
            f"ğŸ“Š Gestos: {self.gesture_count} | Comandos: {self.command_count} | Acciones: {self.multimodal_actions}",
            "",
            "ğŸ® Controles Multimodales:",
            "â€¢ Mano abierta + 'cambiar' = Cambiar color",
            "â€¢ Dos dedos + 'mover' = Activar movimiento", 
            "â€¢ PuÃ±o + 'rotar' = Activar rotaciÃ³n",
            "â€¢ Gesto OK + 'mostrar' = Ver estadÃ­sticas",
            "â€¢ Mano abierta + colores = Cambiar a color especÃ­fico",
            "",
            "ğŸ—£ï¸ Comandos de voz (funcionan solos o con gestos):",
            "â€¢ 'mover' = Activar/desactivar movimiento",
            "â€¢ 'rotar' = Activar/desactivar rotaciÃ³n", 
            "â€¢ 'cambiar', 'mostrar', 'parar', 'reset', 'salir'",
            "â€¢ Colores: azul, rojo, verde, amarillo",
        ]
        
        for text in info_texts:
            if text:  # Solo renderizar lÃ­neas no vacÃ­as
                surface = self.font.render(text, True, COLORS['blanco'])
                self.screen.blit(surface, (10, y_offset))
            y_offset += 25
        
        # Mensaje de estado
        if self.status_message:
            status_surface = self.font.render(self.status_message, True, COLORS['amarillo'])
            self.screen.blit(status_surface, (10, 550))
        
        # Estado del objeto
        obj_info = [
            f"Objeto - Pos: ({self.interactive_object.x}, {self.interactive_object.y})",
            f"Movimiento: {'SÃ­' if self.interactive_object.is_moving else 'No'}",
            f"RotaciÃ³n: {'SÃ­' if self.interactive_object.is_rotating else 'No'}",
            f"RotaciÃ³n actual: {self.interactive_object.rotation:.0f}Â°"
        ]
        
        for i, text in enumerate(obj_info):
            surface = self.font.render(text, True, COLORS['gris'])
            self.screen.blit(surface, (550, 10 + i * 25))
    
    def cleanup(self):
        """Limpia recursos al cerrar"""
        print("ğŸ§¹ Cerrando sistema multimodal...")
        self.voice_recognizer.stop_listening()
        self.cap.release()
        cv2.destroyAllWindows()
        pygame.quit()

def main():
    """FunciÃ³n principal"""
    try:
        print("ğŸ§ª Taller - Interfaces Multimodales: Uniendo Voz y Gestos")
        print("=" * 60)
        
        # Crear y ejecutar interfaz
        interface = MultimodalInterface()
        interface.start()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ Interrumpido por el usuario")
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("ğŸ‘‹ Â¡Gracias por usar el sistema multimodal!")

if __name__ == "__main__":
    main()
