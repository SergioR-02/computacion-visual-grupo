# ðŸ” Pipeline de VisiÃ³n Computacional: DetecciÃ³n, SegmentaciÃ³n y EstimaciÃ³n de Profundidad

ðŸ“… **Fecha:** 2025-01-07 â€“ Fecha de realizaciÃ³n

ðŸŽ¯ **Objetivo del Proyecto:**
Desarrollar un pipeline integrado de visiÃ³n computacional que combine detecciÃ³n de objetos con YOLO, segmentaciÃ³n con SAM (Segment Anything Model), y estimaciÃ³n de profundidad con MiDaS. El objetivo es crear un sistema completo que detecte objetos, los segmente con precisiÃ³n, estime su profundidad, y genere visualizaciones y aplicaciones creativas como efectos de bokeh y anÃ¡lisis de datos.

---

## ðŸ§  Conceptos Aprendidos

### ðŸŽ¯ DetecciÃ³n de Objetos con YOLO

- **DefiniciÃ³n**: YOLO (You Only Look Once) es un algoritmo de detecciÃ³n de objetos en tiempo real que divide la imagen en una cuadrÃ­cula y predice bounding boxes y probabilidades de clase
- **CaracterÃ­sticas principales**:
  - âœ… DetecciÃ³n en una sola pasada de la red neuronal
  - âœ… PredicciÃ³n simultÃ¡nea de mÃºltiples objetos
  - âœ… Salida con coordenadas de bounding boxes y confianza
  - âœ… Soporte para mÃºltiples clases de objetos (80+ en COCO dataset)

### âœ‚ï¸ SegmentaciÃ³n con SAM (Segment Anything Model)

- **Prompt-based Segmentation**: SAM utiliza prompts (puntos, boxes, texto) para generar mÃ¡scaras de segmentaciÃ³n precisas
- **Box Prompting**: Uso de bounding boxes de YOLO como prompts para SAM
- **Mask Generation**: ProducciÃ³n de mÃ¡scaras binarias de alta calidad
- **Zero-shot Capability**: Capacidad de segmentar objetos sin entrenamiento especÃ­fico

### ðŸ“ EstimaciÃ³n de Profundidad con MiDaS

- **Monocular Depth Estimation**: EstimaciÃ³n de profundidad a partir de una sola imagen
- **Relative Depth**: MiDaS produce mapas de profundidad relativa (no absoluta)
- **Multi-scale Processing**: Procesamiento a diferentes escalas para mejor precisiÃ³n
- **Dense Prediction**: Cada pÃ­xel recibe una estimaciÃ³n de profundidad

### ðŸŽ¨ Aplicaciones Creativas y AnalÃ­ticas

- **Efecto Bokeh**: Desenfoque selectivo basado en profundidad
- **Pixelado de Fondo**: AplicaciÃ³n de efectos a regiones no segmentadas
- **AnÃ¡lisis EstadÃ­stico**: ExtracciÃ³n de mÃ©tricas de Ã¡rea, profundidad y distribuciÃ³n
- **ExportaciÃ³n de Datos**: GeneraciÃ³n de reportes en formato CSV

---

## ðŸ”§ Herramientas y Entornos

- **Python 3.8+** - Lenguaje de programaciÃ³n principal
- **PyTorch** - Framework de deep learning para YOLO y SAM
- **OpenCV** - Procesamiento de imÃ¡genes y visualizaciÃ³n
- **Transformers** - Modelos pre-entrenados de Hugging Face
- **NumPy** - Operaciones numÃ©ricas y manipulaciÃ³n de arrays
- **Matplotlib** - VisualizaciÃ³n de resultados y grÃ¡ficos
- **Pillow (PIL)** - Procesamiento adicional de imÃ¡genes
- **Pandas** - AnÃ¡lisis y exportaciÃ³n de datos
- **Google Colab** - Entorno de desarrollo en la nube con GPU
---

## ðŸ“ Estructura del Proyecto

```
Pipeline_Vision_Computacional/
â”œâ”€â”€ deteccion_objetos_yolo.ipynb     # Notebook principal del pipeline
â”œâ”€â”€ README.md                        # Esta documentaciÃ³n
â”œâ”€â”€ informeParte1.md  
â””â”€â”€ resultados/                      # ImÃ¡genes generadas y anÃ¡lisis
```

---

## ðŸ§ª ImplementaciÃ³n

### ðŸ”¹ Etapas realizadas

1. **ConfiguraciÃ³n del entorno**: InstalaciÃ³n automÃ¡tica de dependencias para Colab y local
2. **DetecciÃ³n con YOLO**: ImplementaciÃ³n de YOLOv8 para detectar mÃºltiples clases de objetos
3. **SegmentaciÃ³n con SAM**: Uso de bounding boxes como prompts para generar mÃ¡scaras precisas
4. **EstimaciÃ³n de profundidad**: IntegraciÃ³n de MiDaS para mapas de profundidad densos
5. **VisualizaciÃ³n combinada**: Overlay de detecciones, mÃ¡scaras y profundidad
6. **Aplicaciones creativas**: Efectos de bokeh y pixelado basados en segmentaciÃ³n
7. **AnÃ¡lisis de datos**: ExtracciÃ³n de mÃ©tricas y exportaciÃ³n a CSV

### ðŸ”¹ Funciones principales implementadas

**DetecciÃ³n de objetos con YOLO:**

```python
def detectar_objetos_yolo(imagen_path, modelo_yolo, umbral_confianza=0.5):
    """
    Detecta objetos en una imagen usando YOLO.
    
    Args:
        imagen_path: Ruta a la imagen
        modelo_yolo: Modelo YOLO pre-entrenado
        umbral_confianza: Umbral mÃ­nimo de confianza
    
    Returns:
        tuple: (imagen_original, detecciones, boxes, clases, confianzas)
    """
    imagen = cv2.imread(imagen_path)
    imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)
    
    resultados = modelo_yolo(imagen_rgb, conf=umbral_confianza)
    
    # ExtracciÃ³n de bounding boxes, clases y confianzas
    for resultado in resultados:
        boxes = resultado.boxes.xyxy.cpu().numpy()
        clases = resultado.boxes.cls.cpu().numpy()
        confianzas = resultado.boxes.conf.cpu().numpy()
    
    return imagen_rgb, resultados, boxes, clases, confianzas
```

**SegmentaciÃ³n con SAM usando prompts de YOLO:**

```python
def segmentar_con_sam(imagen, boxes, predictor_sam):
    """
    Realiza segmentaciÃ³n usando SAM con bounding boxes como prompts.
    
    Args:
        imagen: Imagen de entrada (numpy array)
        boxes: Bounding boxes de YOLO
        predictor_sam: Predictor de SAM
    
    Returns:
        list: Lista de mÃ¡scaras de segmentaciÃ³n
    """
    predictor_sam.set_image(imagen)
    mascaras = []
    
    for box in boxes:
        # Convertir box a formato SAM (xyxy)
        input_box = np.array(box).astype(np.int32)
        
        # Generar mÃ¡scara usando el box como prompt
        masks, scores, logits = predictor_sam.predict(
            point_coords=None,
            point_labels=None,
            box=input_box[None, :],
            multimask_output=False,
        )
        
        mascaras.append(masks[0])
    
    return mascaras
```

**EstimaciÃ³n de profundidad con MiDaS:**

```python
def estimar_profundidad_midas(imagen, modelo_midas, transform_midas, device):
    """
    Estima la profundidad de una imagen usando MiDaS.
    
    Args:
        imagen: Imagen de entrada
        modelo_midas: Modelo MiDaS pre-entrenado
        transform_midas: Transformaciones para MiDaS
        device: Dispositivo (CPU/GPU)
    
    Returns:
        numpy.ndarray: Mapa de profundidad normalizado
    """
    # Preprocesar imagen para MiDaS
    input_batch = transform_midas(imagen).to(device)
    
    with torch.no_grad():
        prediction = modelo_midas(input_batch)
        
        # Interpolar a tamaÃ±o original
        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=imagen.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze()
    
    # Normalizar a rango [0, 1]
    depth_map = prediction.cpu().numpy()
    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
    
    return depth_map
```

**Efecto bokeh basado en profundidad:**

```python
def efecto_bokeh_profundidad(imagen, mapa_profundidad, mascaras, intensidad_blur=15):
    """
    Aplica efecto bokeh basado en profundidad y segmentaciÃ³n.
    
    Args:
        imagen: Imagen original
        mapa_profundidad: Mapa de profundidad
        mascaras: MÃ¡scaras de segmentaciÃ³n
        intensidad_blur: Intensidad del desenfoque
    
    Returns:
        numpy.ndarray: Imagen con efecto bokeh
    """
    imagen_bokeh = imagen.copy()
    
    # Crear mÃ¡scara combinada de objetos detectados
    mascara_objetos = np.zeros(imagen.shape[:2], dtype=bool)
    for mascara in mascaras:
        mascara_objetos |= mascara
    
    # Aplicar blur variable basado en profundidad
    imagen_blur = cv2.GaussianBlur(imagen, (intensidad_blur, intensidad_blur), 0)
    
    # Mezclar imagen original y blur basado en profundidad y segmentaciÃ³n
    for y in range(imagen.shape[0]):
        for x in range(imagen.shape[1]):
            if not mascara_objetos[y, x]:  # Fondo
                factor_blur = mapa_profundidad[y, x]
                alpha = np.clip(factor_blur * 1.5, 0, 1)
                imagen_bokeh[y, x] = (1 - alpha) * imagen[y, x] + alpha * imagen_blur[y, x]
    
    return imagen_bokeh.astype(np.uint8)
```

---

## ðŸ“Š Resultados y AnÃ¡lisis

### ðŸ“Œ Imagen Original
![Pipeline de VisiÃ³n Computacional](./resultados/Origin.png)
### ðŸ“Œ Pruebas YOLO

![Pipeline de VisiÃ³n Computacional](./resultados/DetallesYolo.png)
![Pipeline de VisiÃ³n Computacional](./resultados/DeteccionYolo.png)
![Pipeline de VisiÃ³n Computacional](./resultados/DistribucionYolo.png)

- âœ… **PrecisiÃ³n**: DetecciÃ³n exitosa de mÃºltiples clases (personas, vehÃ­culos, objetos)
- âœ… **Bounding Boxes**: Coordenadas precisas 

#### Multiples Objetos
![Pipeline de VisiÃ³n Computacional](./resultados/Cocina.png)


### ðŸ“Œ Pipeline Completo en AcciÃ³n:

![Pipeline de VisiÃ³n Computacional](./resultados/DiagramaPipeline.png)

#### RESULTADO YOLO SAM MiDas
![Pipeline de VisiÃ³n Computacional](./resultados/DiagramaPipeline.png)


![Pipeline de VisiÃ³n Computacional](./resultados/YoloSam.png)
### ðŸŽ¯ Detecciones YOLO:
- âœ… **PrecisiÃ³n**: DetecciÃ³n exitosa de mÃºltiples clases (personas, vehÃ­culos, objetos)
- âœ… **Bounding Boxes**: Coordenadas precisas para usar como prompts SAM
- âœ… **Confianza**: Filtrado por umbral de confianza configurable
- âœ… **Clases**: Soporte para 80+ clases del dataset COCO

![Pipeline de VisiÃ³n Computacional](./resultados/Galeria.png)
### âœ‚ï¸ SegmentaciÃ³n SAM:

- âœ… **MÃ¡scaras precisas**: SegmentaciÃ³n pixel-perfect usando prompts de YOLO
- âœ… **MÃºltiples objetos**: Procesamiento simultÃ¡neo de varios objetos detectados
- âœ… **Alta calidad**: Bordes suaves y precisos en las mÃ¡scaras
- âœ… **Robustez**: Funciona bien con objetos parcialmente ocluidos


![Pipeline de VisiÃ³n Computacional](./resultados/Profundidad.png)
### ðŸ“ Mapas de Profundidad:

- âœ… **Profundidad relativa**: EstimaciÃ³n consistente de distancias relativas
- âœ… **ResoluciÃ³n completa**: Mapas densos para cada pÃ­xel
- âœ… **Coherencia espacial**: Transiciones suaves entre regiones
- âœ… **Compatibilidad**: Funciona con imÃ¡genes de cualquier resoluciÃ³n


![Pipeline de VisiÃ³n Computacional](./resultados/EfectosVisuales.png)
### ðŸŽ¨ Aplicaciones Creativas:

- âœ… **Efecto Bokeh**: Desenfoque realista basado en profundidad
- âœ… **Pixelado selectivo**: Efectos artÃ­sticos en fondo/objetos
- âœ… **Recortes precisos**: ExtracciÃ³n de objetos segmentados
- âœ… **GalerÃ­a interactiva**: VisualizaciÃ³n organizada de resultados



![Pipeline de VisiÃ³n Computacional](./resultados/Clasificacion.png)
### ðŸ“ˆ AnÃ¡lisis EstadÃ­stico:

- âœ… **MÃ©tricas de Ã¡rea**: CÃ¡lculo de Ã¡reas de objetos segmentados
- âœ… **AnÃ¡lisis de profundidad**: EstadÃ­sticas de distribuciÃ³n de profundidad
- âœ… **ExportaciÃ³n CSV**: Datos estructurados para anÃ¡lisis posterior
- âœ… **Visualizaciones**: GrÃ¡ficos de distribuciÃ³n y comparativas

---

## ðŸ”§ ConfiguraciÃ³n y Uso

### ðŸ Requisitos del Sistema

```bash
# InstalaciÃ³n automÃ¡tica en Colab
!pip install ultralytics segment-anything transformers opencv-python matplotlib pillow pandas

# Para uso local
pip install torch torchvision ultralytics segment-anything transformers opencv-python matplotlib pillow pandas
```

### ðŸš€ EjecuciÃ³n RÃ¡pida

1. **Abrir el notebook**: `deteccion_objetos_yolo.ipynb`
2. **Ejecutar configuraciÃ³n**: Primera celda instala dependencias automÃ¡ticamente
3. **Cargar imagen**: Subir imagen de prueba o usar una de ejemplo
4. **Ejecutar pipeline**: Correr todas las celdas secuencialmente
5. **Explorar resultados**: Visualizar detecciones, segmentaciones y efectos

### ðŸŽ›ï¸ ParÃ¡metros Configurables

- **Umbral de confianza YOLO**: Ajustar sensibilidad de detecciÃ³n
- **Intensidad de bokeh**: Controlar nivel de desenfoque
- **TamaÃ±o de pÃ­xeles**: Modificar granularidad del pixelado
- **Clases a detectar**: Filtrar por tipos especÃ­ficos de objetos
- **ResoluciÃ³n de salida**: Ajustar calidad de imÃ¡genes generadas

---

## ðŸ’¡ Casos de Uso

### ðŸ™ï¸ FotografÃ­a Urbana
- DetecciÃ³n de vehÃ­culos y peatones
- Efectos de profundidad en escenas complejas
- AnÃ¡lisis de distribuciÃ³n espacial de objetos

### ðŸ  Interiores y Arquitectura
- SegmentaciÃ³n de muebles y decoraciÃ³n
- AnÃ¡lisis de profundidad espacial
- Efectos artÃ­sticos en espacios

### ðŸŒ¿ Naturaleza y Paisajes
- DetecciÃ³n de animales y objetos naturales
- Efectos de bokeh en fotografÃ­a macro
- AnÃ¡lisis de composiciÃ³n de escenas

### ðŸ“± Redes Sociales y Arte Digital
- Efectos creativos automatizados
- SegmentaciÃ³n para composiciones
- GeneraciÃ³n de contenido visual

---

## ðŸ”¬ AnÃ¡lisis TÃ©cnico

### âš¡ Rendimiento

- **YOLO**: ~50ms por imagen en GPU, ~200ms en CPU
- **SAM**: ~100ms por objeto en GPU, ~500ms en CPU  
- **MiDaS**: ~300ms por imagen en GPU, ~2s en CPU
- **Pipeline completo**: ~1-3 segundos dependiendo del hardware

### ðŸŽ¯ PrecisiÃ³n

- **DetecciÃ³n YOLO**: mAP@0.5 > 0.7 en dataset COCO
- **SegmentaciÃ³n SAM**: IoU > 0.85 con prompts de calidad
- **Profundidad MiDaS**: Error relativo < 15% en escenas tÃ­picas

### ðŸ› ï¸ Limitaciones Identificadas

- **Objetos muy pequeÃ±os**: YOLO puede fallar en detecciÃ³n
- **OclusiÃ³n severa**: SAM tiene dificultades con objetos muy ocluidos
- **Escenas complejas**: MiDaS puede confundirse con reflejos/transparencias
- **Tiempo real**: El pipeline completo no es apto para aplicaciones en vivo

---

## ðŸ’¬ ReflexiÃ³n Final

Este proyecto nos permitiÃ³ comprender la integraciÃ³n de mÃºltiples modelos de inteligencia artificial dentro de un pipeline cohesivo. Integramos tecnologÃ­as como YOLO, SAM y MiDaS, demostrando cÃ³mo distintas especializaciones pueden complementarse para construir aplicaciones mÃ¡s ricas y funcionales.

ðŸ”‘ **Aspectos Clave Aprendidos como Grupo**:

* **Prompting estratÃ©gico**: Utilizamos las salidas de un modelo como entradas de otro para maximizar la precisiÃ³n del sistema.
* **GestiÃ³n de memoria**: Al trabajar con modelos de gran tamaÃ±o, fue esencial optimizar cuidadosamente los recursos disponibles.
* **ValidaciÃ³n robusta**: Implementamos un manejo de errores sÃ³lido, fundamental al integrar mÃºltiples sistemas interdependientes.
* **VisualizaciÃ³n efectiva**: DiseÃ±amos presentaciones claras de los resultados, reconociendo que la forma de mostrar los datos es tan importante como los algoritmos en sÃ­.

ðŸš€ **Aplicaciones Futuras Identificadas**:

* **Realidad aumentada**: SegmentaciÃ³n en tiempo real para enriquecer experiencias AR.
* **RobÃ³tica**: Sistemas de navegaciÃ³n y manipulaciÃ³n guiados por profundidad y segmentaciÃ³n.
* **AnÃ¡lisis mÃ©dico**: DetecciÃ³n y segmentaciÃ³n de estructuras anatÃ³micas para estimaciones volumÃ©tricas.
* **AutomociÃ³n**: Desarrollo de sistemas avanzados de asistencia al conductor (ADAS).

ðŸŽ“ **Valor AcadÃ©mico del Trabajo**:
Durante el desarrollo, aplicamos colectivamente conocimientos clave en:

* **Deep Learning**: Arquitecturas CNN, transferencia de aprendizaje y fine-tuning.
* **VisiÃ³n por Computador**: DetecciÃ³n, segmentaciÃ³n y estimaciÃ³n de profundidad.
* **Procesamiento de ImÃ¡genes**: AplicaciÃ³n de filtros, transformaciones y anÃ¡lisis estadÃ­stico.
* **IngenierÃ­a de Software**: DiseÃ±o de pipelines robustos, manejo de errores y modularidad del cÃ³digo.

La implementaciÃ³n prÃ¡ctica de este sistema evidenciÃ³ que la visiÃ³n computacional moderna no se basa en un Ãºnico algoritmo, sino en la orquestaciÃ³n inteligente de mÃºltiples sistemas especializados, trabajando en conjunto hacia un mismo objetivo.

ðŸŽ¨ **Otras Ãreas de AplicaciÃ³n Exploradas**:

* **Arquitectura virtual**: AplicaciÃ³n precisa de texturas sobre modelos tridimensionales.
* **Arte digital**: GeneraciÃ³n de efectos visuales a travÃ©s de manipulaciÃ³n de coordenadas UV.
* **Realidad aumentada**: Mapeo exacto de texturas sobre objetos fÃ­sicos del entorno.