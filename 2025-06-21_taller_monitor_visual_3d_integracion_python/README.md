# ğŸ” Creando un Monitor de Actividad Visual en 3D

ğŸ“… **Fecha:** 2025-06-21 â€“ Fecha de realizaciÃ³n

ğŸ¯ **Objetivo del Proyecto:**
DiseÃ±ar una escena 3D interactiva que se adapte en tiempo real segÃºn los datos provenientes de un sistema de visiÃ³n por computador. La escena debe responder visualmente (cambiando color, escala o posiciÃ³n de objetos) en funciÃ³n de mÃ©tricas detectadas, simulando asÃ­ un sistema de vigilancia, arte generativo reactivo o interfaz inteligente.

---

## ğŸ§  Conceptos Aprendidos

### ğŸ¯ DetecciÃ³n de Objetos con YOLO

- **DefiniciÃ³n**: YOLO (You Only Look Once) es un algoritmo de detecciÃ³n de objetos en tiempo real que divide la imagen en una cuadrÃ­cula y predice bounding boxes y probabilidades de clase
- **CaracterÃ­sticas principales**:
  - âœ… DetecciÃ³n en una sola pasada de la red neuronal
  - âœ… PredicciÃ³n simultÃ¡nea de mÃºltiples objetos
  - âœ… Salida con coordenadas de bounding boxes y confianza
  - âœ… Soporte para mÃºltiples clases de objetos (80+ en COCO dataset)

### ğŸ¤– DetecciÃ³n de Poses y Gestos con MediaPipe

- **Pose Detection**: DetecciÃ³n de 33 puntos clave del cuerpo humano en tiempo real
- **Hand Tracking**: Seguimiento de manos con 21 landmarks por mano
- **Face Detection**: DetecciÃ³n facial con bounding boxes y puntos de referencia
- **Gesture Recognition**: AnÃ¡lisis bÃ¡sico de gestos (puÃ±o, mano abierta, seÃ±alar, etc.)

### ğŸŒ ComunicaciÃ³n en Tiempo Real con WebSockets

- **Protocolo WebSocket**: ComunicaciÃ³n bidireccional de baja latencia
- **Streaming de Datos**: TransmisiÃ³n continua de mÃ©tricas visuales
- **CompresiÃ³n de Datos**: OptimizaciÃ³n del payload para mejor rendimiento
- **ReconexiÃ³n AutomÃ¡tica**: Manejo robusto de desconexiones

### ğŸ¨ VisualizaciÃ³n 3D Reactiva

- **Three.js + React**: IntegraciÃ³n de grÃ¡ficos 3D con React usando @react-three/fiber
- **Objetos Reactivos**: GeometrÃ­as que cambian escala, color y posiciÃ³n segÃºn datos
- **Animaciones Fluidas**: Transiciones suaves entre estados visuales
- **Controles Interactivos**: Panel de configuraciÃ³n en tiempo real con Leva

---

## ğŸ”§ Herramientas y Entornos

### ğŸ Backend (Python)

- **Python 3.8+** - Lenguaje de programaciÃ³n principal
- **YOLO (Ultralytics)** - DetecciÃ³n de objetos en tiempo real
- **MediaPipe** - DetecciÃ³n de poses, manos y rostros
- **OpenCV** - Procesamiento de imÃ¡genes y captura de video
- **WebSockets** - ComunicaciÃ³n en tiempo real
- **NumPy** - Operaciones numÃ©ricas y manipulaciÃ³n de arrays
- **AsyncIO** - ProgramaciÃ³n asÃ­ncrona para mejor rendimiento

### ğŸŒ Frontend (Three.js + React)

- **React 18** - Framework de interfaz de usuario
- **Three.js** - Motor de grÃ¡ficos 3D
- **@react-three/fiber** - IntegraciÃ³n React + Three.js
- **@react-three/drei** - Componentes y helpers para Three.js
- **TypeScript** - Tipado estÃ¡tico para mejor desarrollo
- **Vite** - Herramienta de build rÃ¡pida
- **Leva** - Panel de controles interactivos

### ğŸ”§ Herramientas de Desarrollo

- **Bun** - Runtime y gestor de paquetes rÃ¡pido
- **ESLint** - Linter para cÃ³digo JavaScript/TypeScript
- **WebSocket API** - Protocolo de comunicaciÃ³n bidireccional

---

## ğŸ“ Estructura del Proyecto

```
2025-06-21_taller_monitor_visual_3d_integracion_python/
â”œâ”€â”€ python/                          # Backend de visiÃ³n por computador
â”‚   â”œâ”€â”€ main.py                      # Sistema principal de detecciÃ³n
â”‚   â”œâ”€â”€ requirements.txt             # Dependencias de Python
â”‚   â””â”€â”€ yolov8n.pt                   # Modelo YOLO pre-entrenado
â”œâ”€â”€ threejs/                         # Frontend de visualizaciÃ³n 3D
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx                  # Componente principal
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.ts      # Hook para comunicaciÃ³n WebSocket
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ 3D/                  # Componentes de la escena 3D
â”‚   â”‚       â”‚   â”œâ”€â”€ Scene3D.tsx      # Escena principal
â”‚   â”‚       â”‚   â”œâ”€â”€ ReactiveObject.tsx # Objetos reactivos
â”‚   â”‚       â”‚   â””â”€â”€ MovementParticles.tsx # Sistema de partÃ­culas
â”‚   â”‚       â””â”€â”€ UI/                  # Componentes de interfaz
â”‚   â”‚           â”œâ”€â”€ Header.tsx       # Cabecera con estado
â”‚   â”‚           â”œâ”€â”€ FloatingToolbar.tsx # Barra de herramientas
â”‚   â”‚           â””â”€â”€ InfoOverlay.tsx  # Panel de informaciÃ³n
â”‚   â”œâ”€â”€ package.json                 # Dependencias de Node.js
â”‚   â””â”€â”€ index.html                   # Punto de entrada HTML
â”œâ”€â”€ results/
â”‚   â””â”€â”€ visualization_3D_monitor.gif # DemostraciÃ³n del sistema
â””â”€â”€ README.md                        # Esta documentaciÃ³n
```

---

## ğŸ§ª ImplementaciÃ³n

### ğŸ”¹ Arquitectura del Sistema

El sistema estÃ¡ dividido en dos partes principales que se comunican en tiempo real:

1. **Backend Python**: Captura video, procesa con IA y envÃ­a datos via WebSocket
2. **Frontend Three.js**: Recibe datos y visualiza en escena 3D interactiva

### ğŸ”¹ Backend - Sistema de DetecciÃ³n Visual (Python)

**Clase principal VisualMonitor:**

```python
class VisualMonitor:
    def __init__(self):
        """Inicializa el monitor visual con todos los detectores optimizado"""
        # Modelos de detecciÃ³n
        self.yolo_model = YOLO('yolov8n.pt')  # Modelo ligero

        # MediaPipe para detecciÃ³n de poses y manos (configuraciÃ³n optimizada)
        self.mp_pose = mp.solutions.pose
        self.mp_hands = mp.solutions.hands
        self.mp_face = mp.solutions.face_detection

        # ConfiguraciÃ³n de cÃ¡mara optimizada
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)  # ResoluciÃ³n optimizada
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
```

**DetecciÃ³n de objetos con YOLO:**

```python
def detect_objects_yolo(self, frame: np.ndarray) -> Dict:
    """Detecta objetos usando YOLO"""
    results = self.yolo_model(frame, verbose=False)

    people_count = 0
    objects_count = 0
    detections = []

    for result in results:
        boxes = result.boxes
        if boxes is not None:
            for box in boxes:
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])

                if confidence > 0.5:  # Umbral de confianza
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    class_name = self.yolo_model.names[class_id]

                    detection = {
                        'class': class_name,
                        'confidence': confidence,
                        'bbox': [float(x1), float(y1), float(x2), float(y2)],
                        'center': [float((x1 + x2) / 2), float((y1 + y2) / 2)],
                        'area': float((x2 - x1) * (y2 - y1))
                    }
                    detections.append(detection)

                    if class_name == 'person':
                        people_count += 1
                    else:
                        objects_count += 1

    return {
        'people_count': people_count,
        'objects_count': objects_count,
        'detections': detections
    }
```

**DetecciÃ³n de poses y gestos con MediaPipe:**

```python
def detect_hands_mediapipe(self, frame: np.ndarray) -> Dict:
    """Detecta manos usando MediaPipe"""
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = self.hands.process(rgb_frame)

    hands_data = {
        'hands_count': 0,
        'hands_landmarks': [],
        'gestures': []
    }

    if results.multi_hand_landmarks:
        hands_data['hands_count'] = len(results.multi_hand_landmarks)

        for hand_landmarks in results.multi_hand_landmarks:
            landmarks = []
            for landmark in hand_landmarks.landmark:
                landmarks.extend([landmark.x, landmark.y, landmark.z])
            hands_data['hands_landmarks'].append(landmarks)

            # AnÃ¡lisis bÃ¡sico de gestos (puÃ±o cerrado vs abierto)
            gesture = self.analyze_hand_gesture(hand_landmarks.landmark)
            hands_data['gestures'].append(gesture)

    return hands_data
```

**Servidor WebSocket asÃ­ncrono:**

```python
async def websocket_handler(self, websocket):
    """Maneja conexiones WebSocket"""
    self.connected_clients.add(websocket)

    # Mensaje de bienvenida
    await websocket.send(json.dumps({
        'type': 'connection',
        'message': 'ğŸ¯ Conectado al Monitor Visual 3D',
        'timestamp': time.time()
    }))

    try:
        await websocket.wait_closed()
    finally:
        self.connected_clients.discard(websocket)
```

### ğŸ”¹ Frontend - VisualizaciÃ³n 3D (Three.js + React)

**Hook para comunicaciÃ³n WebSocket:**

```typescript
export const useWebSocket = (url: string) => {
  const [data, setData] = useState<VisualData | null>(null);
  const [connected, setConnected] = useState(false);
  const ws = useRef<WebSocket | null>(null);

  useEffect(() => {
    const connect = () => {
      ws.current = new WebSocket(url);

      ws.current.onopen = () => {
        console.log('ğŸ”Œ ConexiÃ³n WebSocket establecida');
        setConnected(true);
      };

      ws.current.onmessage = event => {
        try {
          const receivedData = JSON.parse(event.data);
          if (receivedData.timestamp) {
            setData(receivedData);
          }
        } catch (e) {
          console.error('âŒ Error parseando datos:', e);
        }
      };
    };

    connect();
  }, [url]);

  return { data, connected };
};
```

**Componente de escena 3D reactiva:**

```tsx
export const Scene3D: React.FC<Scene3DProps> = ({ data }) => {
  // Escalas basadas en datos en tiempo real
  const peopleScale = Math.max(0.3, data.people_count * 0.3);
  const objectsScale = Math.max(0.3, data.objects_count * 0.2);
  const movementScale = Math.max(0.2, data.movement_intensity * 2);
  const handsScale = Math.max(0.2, data.hands_count * 0.4);

  return (
    <group>
      {/* Objetos reactivos que cambian segÃºn los datos */}
      <ReactiveObject
        position={[-3, 0, 0]}
        scale={peopleScale}
        color="#ff0080"
        type="sphere"
        intensity={data.movement_intensity}
        label={`ğŸ‘¥ Personas: ${data.people_count}`}
        animated={true}
      />

      <ReactiveObject
        position={[3, 0, 0]}
        scale={objectsScale}
        color="#00ff80"
        type="box"
        intensity={data.movement_intensity}
        label={`ğŸ“¦ Objetos: ${data.objects_count}`}
        animated={true}
      />

      <ReactiveObject
        position={[0, 2, 0]}
        scale={movementScale}
        color="#8000ff"
        type="cone"
        intensity={data.movement_intensity}
        label={`ğŸƒ Movimiento: ${(data.movement_intensity * 100).toFixed(1)}%`}
        animated={true}
      />
    </group>
  );
};
```

**Objetos 3D reactivos:**

```tsx
export const ReactiveObject: React.FC<ReactiveObjectProps> = ({
  position,
  scale,
  color,
  type,
  intensity,
  animated,
}) => {
  const meshRef = useRef<THREE.Mesh>(null);

  // AnimaciÃ³n continua basada en intensidad
  useFrame((state, delta) => {
    if (meshRef.current && animated) {
      meshRef.current.rotation.y += delta * intensity * 2;
      meshRef.current.position.y =
        position[1] + Math.sin(state.clock.elapsedTime * 2) * intensity * 0.5;
    }
  });

  const geometry =
    type === 'sphere' ? (
      <sphereGeometry args={[1, 32, 32]} />
    ) : type === 'box' ? (
      <boxGeometry args={[1, 1, 1]} />
    ) : type === 'cone' ? (
      <coneGeometry args={[1, 2, 8]} />
    ) : (
      <cylinderGeometry args={[1, 1, 1, 8]} />
    );

  return (
    <mesh
      ref={meshRef}
      position={position}
      scale={scale}
      castShadow
      receiveShadow
    >
      {geometry}
      <meshStandardMaterial color={color} metalness={0.7} roughness={0.3} />
    </mesh>
  );
};
```

---

## ğŸ“Š Resultados y DemostraciÃ³n

### ğŸ¬ DemostraciÃ³n del Sistema Completo

![Monitor Visual 3D en Funcionamiento](./results/visualization_3D_monitor.gif)

_El sistema completo funcionando: detecciÃ³n en tiempo real con Python (izquierda) y visualizaciÃ³n 3D reactiva con Three.js (derecha)_

### ğŸ” CaracterÃ­sticas del Sistema Implementado

#### ğŸ¯ Backend Python - DetecciÃ³n en Tiempo Real:

- âœ… **DetecciÃ³n YOLO**: IdentificaciÃ³n de personas y objetos con YOLOv8n
- âœ… **Seguimiento de manos**: DetecciÃ³n y anÃ¡lisis de gestos con MediaPipe
- âœ… **DetecciÃ³n de poses**: 33 puntos clave del cuerpo humano
- âœ… **AnÃ¡lisis de movimiento**: CÃ¡lculo de intensidad de movimiento por frame
- âœ… **OptimizaciÃ³n de rendimiento**: Procesamiento alternado para mantener 30 FPS
- âœ… **Streaming WebSocket**: TransmisiÃ³n de datos en tiempo real

#### ğŸ¨ Frontend Three.js - VisualizaciÃ³n 3D Interactiva:

- âœ… **Objetos reactivos**: GeometrÃ­as que cambian escala segÃºn detecciones
- âœ… **Animaciones fluidas**: RotaciÃ³n y movimiento basado en intensidad
- âœ… **Esquemas de color**: MÃºltiples paletas (neon, pastel, vibrante, monocromo)
- âœ… **Controles interactivos**: Panel de configuraciÃ³n en tiempo real con Leva
- âœ… **IluminaciÃ³n dinÃ¡mica**: Ambiente que responde a los datos
- âœ… **InformaciÃ³n en tiempo real**: Overlay con mÃ©tricas actualizadas

#### ğŸŒ ComunicaciÃ³n y SincronizaciÃ³n:

- âœ… **WebSocket bidireccional**: Latencia < 50ms entre detecciÃ³n y visualizaciÃ³n
- âœ… **ReconexiÃ³n automÃ¡tica**: Manejo robusto de desconexiones
- âœ… **CompresiÃ³n de datos**: OptimizaciÃ³n del payload para mejor rendimiento
- âœ… **SincronizaciÃ³n temporal**: Timestamps para coherencia de datos

### ğŸ“ˆ MÃ©tricas de Rendimiento

| Componente               | MÃ©trica                     | Valor              |
| ------------------------ | --------------------------- | ------------------ |
| **DetecciÃ³n YOLO**       | Tiempo de procesamiento     | ~30-50ms por frame |
| **MediaPipe Hands**      | Tiempo de procesamiento     | ~15-25ms por frame |
| **MediaPipe Pose**       | Tiempo de procesamiento     | ~20-30ms por frame |
| **WebSocket**            | Latencia de transmisiÃ³n     | ~5-15ms            |
| **Renderizado 3D**       | FPS objetivo                | 60 FPS             |
| **ResoluciÃ³n de cÃ¡mara** | Optimizada para rendimiento | 320x240 @ 30 FPS   |

### ğŸ® Interactividad y Controles

- **ğŸ›ï¸ Panel de Control**: ConfiguraciÃ³n en tiempo real de escalas, colores y efectos
- **ğŸ–±ï¸ Controles de cÃ¡mara**: Orbit, zoom y pan en la escena 3D
- **ğŸ”„ Modo pantalla completa**: VisualizaciÃ³n inmersiva
- **ğŸ“Š MÃ©tricas en vivo**: Contador de FPS y estadÃ­sticas de rendimiento
- **ğŸ¨ Esquemas visuales**: Cambio dinÃ¡mico de paletas de colores

---

## ğŸ”§ ConfiguraciÃ³n y Uso

### ğŸ Requisitos del Sistema

**Backend Python:**

```bash
# Instalar dependencias de Python
pip install -r python/requirements.txt

# O instalar manualmente:
pip install opencv-python>=4.8.0 ultralytics>=8.1.0 websockets>=11.0.0 numpy>=1.24.0 mediapipe>=0.10.21
```

**Frontend Three.js:**

```bash
# Navegar al directorio del frontend
cd threejs/

# Instalar dependencias (usando Bun - mÃ¡s rÃ¡pido)
bun install

# O usar npm
npm install
```

### ğŸš€ EjecuciÃ³n del Sistema Completo

#### 1. **Iniciar el Backend Python**

```bash
cd python/
python main.py
```

_El sistema iniciarÃ¡ la cÃ¡mara y el servidor WebSocket en el puerto 8765_

#### 2. **Iniciar el Frontend Three.js**

```bash
cd threejs/
bun dev
# O usar npm dev
```

_La aplicaciÃ³n web estarÃ¡ disponible en http://localhost:5173_

#### 3. **Verificar ConexiÃ³n**

- âœ… El backend debe mostrar: "ğŸ¯ Monitor Visual OPTIMIZADO inicializado"
- âœ… El frontend debe mostrar: "ğŸ”Œ ConexiÃ³n WebSocket establecida"
- âœ… Los datos deben fluir en tiempo real entre ambos sistemas

### ğŸ›ï¸ Controles y ConfiguraciÃ³n

#### ğŸ”§ Panel de Control (Leva)

- **ConfiguraciÃ³n General**:

  - `scaleMultiplier`: Multiplicador global de escalas (0.1 - 3.0)
  - `autoRotate`: RotaciÃ³n automÃ¡tica de la cÃ¡mara
  - `cameraDistance`: Distancia de la cÃ¡mara (5 - 20)

- **VisualizaciÃ³n**:

  - `enableParticles`: Activar sistema de partÃ­culas
  - `showMetrics`: Mostrar mÃ©tricas en pantalla
  - `colorScheme`: Esquema de colores (neon, pastel, vibrante, monocromo)

- **IluminaciÃ³n**:
  - `lightIntensity`: Intensidad de luces (0.1 - 3.0)
  - `environmentPreset`: Ambiente predefinido (sunset, dawn, night, etc.)

#### âŒ¨ï¸ Controles de CÃ¡mara

- **RatÃ³n izquierdo**: Rotar vista
- **RatÃ³n derecho**: Hacer pan
- **Rueda del ratÃ³n**: Zoom in/out
- **Doble clic**: Resetear vista

#### ğŸ¯ ConfiguraciÃ³n Backend

Modificar en `python/main.py`:

```python
# ResoluciÃ³n de cÃ¡mara (mayor = mejor calidad, menor rendimiento)
self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)   # Cambiar a 640 para HD
self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)  # Cambiar a 480 para HD

# Intervalos de procesamiento (menor = mÃ¡s frecuente, mayor CPU)
self.yolo_interval = 3      # Procesar YOLO cada N frames
self.pose_interval = 2      # Procesar pose cada N frames
self.hands_interval = 2     # Procesar manos cada N frames
```

### ğŸ› ï¸ SoluciÃ³n de Problemas

#### ğŸ” Problemas Comunes

**âŒ Error: "No se puede conectar a WebSocket"**

- Verificar que el backend Python estÃ© ejecutÃ¡ndose
- Confirmar que el puerto 8765 estÃ© libre
- Revisar firewall/antivirus

**âŒ Error: "No se detecta cÃ¡mara"**

- Verificar que la cÃ¡mara estÃ© conectada y funcionando
- Probar cambiar el Ã­ndice de cÃ¡mara: `cv2.VideoCapture(1)` en lugar de `cv2.VideoCapture(0)`
- Cerrar otras aplicaciones que puedan estar usando la cÃ¡mara

**âŒ Rendimiento lento**

- Reducir resoluciÃ³n de cÃ¡mara
- Aumentar intervalos de procesamiento
- Cerrar aplicaciones innecesarias
- Verificar que se estÃ© usando GPU si estÃ¡ disponible

**âŒ Error: "Module not found"**

- Verificar que todas las dependencias estÃ©n instaladas
- Usar entorno virtual de Python
- Reinstalar dependencias con `pip install --upgrade`

---

## ğŸ’¬ ReflexiÃ³n Final

Este proyecto representa un logro significativo en la integraciÃ³n de tecnologÃ­as de visiÃ³n por computador con visualizaciÃ³n 3D interactiva. Logramos crear un sistema completo que combina detecciÃ³n inteligente (YOLO + MediaPipe) con visualizaciÃ³n inmersiva (Three.js + React), conectados mediante WebSockets para comunicaciÃ³n en tiempo real. Los aspectos clave incluyen arquitectura distribuida, optimizaciÃ³n de rendimiento para mantener 30+ FPS, y la creaciÃ³n de objetos 3D reactivos que responden dinÃ¡micamente a los datos de IA.

ğŸš€ **Innovaciones TÃ©cnicas**: Implementamos procesamiento alternado para ejecutar mÃºltiples modelos de IA sin degradar rendimiento, sistema de reconexiÃ³n robusto para mayor estabilidad, y controles interactivos avanzados que permiten configuraciÃ³n en tiempo real. La combinaciÃ³n de AsyncIO en Python con React Three Fiber demostrÃ³ ser altamente efectiva para crear experiencias fluidas e inmersivas.

ğŸŒŸ **Impacto y Futuro**: Este sistema demuestra que es posible crear aplicaciones de IA accesibles y visualmente atractivas sin sacrificar rendimiento. Abre nuevas posibilidades para interfaces naturales, monitoreo inteligente, arte generativo y educaciÃ³n interactiva. El proyecto sienta las bases para sistemas mÃ¡s avanzados que podrÃ­an incluir mÃºltiples cÃ¡maras, IA mÃ¡s sofisticada con comandos de voz, realidad aumentada y colaboraciÃ³n remota, demostrando que el futuro estÃ¡ en la integraciÃ³n inteligente de mÃºltiples tecnologÃ­as.
