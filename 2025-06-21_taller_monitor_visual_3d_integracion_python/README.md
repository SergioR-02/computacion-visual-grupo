# üîç Creando un Monitor de Actividad Visual en 3D

üìÖ **Fecha:** 2025-06-21 ‚Äì Fecha de realizaci√≥n

üéØ **Objetivo del Proyecto:**
Dise√±ar una escena 3D interactiva que se adapte en tiempo real seg√∫n los datos provenientes de un sistema de visi√≥n por computador. La escena debe responder visualmente (cambiando color, escala o posici√≥n de objetos) en funci√≥n de m√©tricas detectadas, simulando as√≠ un sistema de vigilancia, arte generativo reactivo o interfaz inteligente.

---

## üß† Conceptos Aprendidos

### üéØ Detecci√≥n de Objetos con YOLO

- **Definici√≥n**: YOLO (You Only Look Once) es un algoritmo de detecci√≥n de objetos en tiempo real que divide la imagen en una cuadr√≠cula y predice bounding boxes y probabilidades de clase
- **Caracter√≠sticas principales**:
  - ‚úÖ Detecci√≥n en una sola pasada de la red neuronal
  - ‚úÖ Predicci√≥n simult√°nea de m√∫ltiples objetos
  - ‚úÖ Salida con coordenadas de bounding boxes y confianza
  - ‚úÖ Soporte para m√∫ltiples clases de objetos (80+ en COCO dataset)

### ü§ñ Detecci√≥n de Poses y Gestos con MediaPipe

- **Pose Detection**: Detecci√≥n de 33 puntos clave del cuerpo humano en tiempo real
- **Hand Tracking**: Seguimiento de manos con 21 landmarks por mano
- **Face Detection**: Detecci√≥n facial con bounding boxes y puntos de referencia
- **Gesture Recognition**: An√°lisis b√°sico de gestos (pu√±o, mano abierta, se√±alar, etc.)

### üåê Comunicaci√≥n en Tiempo Real con WebSockets

- **Protocolo WebSocket**: Comunicaci√≥n bidireccional de baja latencia
- **Streaming de Datos**: Transmisi√≥n continua de m√©tricas visuales
- **Compresi√≥n de Datos**: Optimizaci√≥n del payload para mejor rendimiento
- **Reconexi√≥n Autom√°tica**: Manejo robusto de desconexiones

### üé® Visualizaci√≥n 3D Reactiva

- **Three.js + React**: Integraci√≥n de gr√°ficos 3D con React usando @react-three/fiber
- **Objetos Reactivos**: Geometr√≠as que cambian escala, color y posici√≥n seg√∫n datos
- **Animaciones Fluidas**: Transiciones suaves entre estados visuales
- **Controles Interactivos**: Panel de configuraci√≥n en tiempo real con Leva

---

## üîß Herramientas y Entornos

### üêç Backend (Python)

- **Python 3.8+** - Lenguaje de programaci√≥n principal
- **YOLO (Ultralytics)** - Detecci√≥n de objetos en tiempo real
- **MediaPipe** - Detecci√≥n de poses, manos y rostros
- **OpenCV** - Procesamiento de im√°genes y captura de video
- **WebSockets** - Comunicaci√≥n en tiempo real
- **NumPy** - Operaciones num√©ricas y manipulaci√≥n de arrays
- **AsyncIO** - Programaci√≥n as√≠ncrona para mejor rendimiento

### üåê Frontend (Three.js + React)

- **React 18** - Framework de interfaz de usuario
- **Three.js** - Motor de gr√°ficos 3D
- **@react-three/fiber** - Integraci√≥n React + Three.js
- **@react-three/drei** - Componentes y helpers para Three.js
- **TypeScript** - Tipado est√°tico para mejor desarrollo
- **Vite** - Herramienta de build r√°pida
- **Leva** - Panel de controles interactivos

### üîß Herramientas de Desarrollo

- **Bun** - Runtime y gestor de paquetes r√°pido
- **ESLint** - Linter para c√≥digo JavaScript/TypeScript
- **WebSocket API** - Protocolo de comunicaci√≥n bidireccional

---

## üìÅ Estructura del Proyecto

```
2025-06-21_taller_monitor_visual_3d_integracion_python/
‚îú‚îÄ‚îÄ python/                          # Backend de visi√≥n por computador
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # Sistema principal de detecci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt             # Dependencias de Python
‚îÇ   ‚îî‚îÄ‚îÄ yolov8n.pt                   # Modelo YOLO pre-entrenado
‚îú‚îÄ‚îÄ threejs/                         # Frontend de visualizaci√≥n 3D
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx                  # Componente principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useWebSocket.ts      # Hook para comunicaci√≥n WebSocket
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 3D/                  # Componentes de la escena 3D
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Scene3D.tsx      # Escena principal
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ReactiveObject.tsx # Objetos reactivos
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ MovementParticles.tsx # Sistema de part√≠culas
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ UI/                  # Componentes de interfaz
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ Header.tsx       # Cabecera con estado
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ FloatingToolbar.tsx # Barra de herramientas
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ InfoOverlay.tsx  # Panel de informaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ package.json                 # Dependencias de Node.js
‚îÇ   ‚îî‚îÄ‚îÄ index.html                   # Punto de entrada HTML
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îî‚îÄ‚îÄ visualization_3D_monitor.gif # Demostraci√≥n del sistema
‚îî‚îÄ‚îÄ README.md                        # Esta documentaci√≥n
```

---

## üß™ Implementaci√≥n

### üîπ Arquitectura del Sistema

El sistema est√° dividido en dos partes principales que se comunican en tiempo real:

1. **Backend Python**: Captura video, procesa con IA y env√≠a datos via WebSocket
2. **Frontend Three.js**: Recibe datos y visualiza en escena 3D interactiva

### üîπ Backend - Sistema de Detecci√≥n Visual (Python)

**Clase principal VisualMonitor:**

```python
class VisualMonitor:
    def __init__(self):
        """Inicializa el monitor visual con todos los detectores optimizado"""
        # Modelos de detecci√≥n
        self.yolo_model = YOLO('yolov8n.pt')  # Modelo ligero

        # MediaPipe para detecci√≥n de poses y manos (configuraci√≥n optimizada)
        self.mp_pose = mp.solutions.pose
        self.mp_hands = mp.solutions.hands
        self.mp_face = mp.solutions.face_detection

        # Configuraci√≥n de c√°mara optimizada
        self.cap = cv2.VideoCapture(0)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)  # Resoluci√≥n optimizada
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
```

**Detecci√≥n de objetos con YOLO:**

```python
def detect_objects_yolo(self, frame: np.ndarray) -> Dict:
    """Detecta objetos usando YOLO"""
    results = self.yolo_model(frame, verbose=False)

    people_count = 0
    objects_count = 0
    detections = []

    for result in results:
        boxes = result.boxes
        if boxes is not None:
            for box in boxes:
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])

                if confidence > 0.5:  # Umbral de confianza
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    class_name = self.yolo_model.names[class_id]

                    detection = {
                        'class': class_name,
                        'confidence': confidence,
                        'bbox': [float(x1), float(y1), float(x2), float(y2)],
                        'center': [float((x1 + x2) / 2), float((y1 + y2) / 2)],
                        'area': float((x2 - x1) * (y2 - y1))
                    }
                    detections.append(detection)

                    if class_name == 'person':
                        people_count += 1
                    else:
                        objects_count += 1

    return {
        'people_count': people_count,
        'objects_count': objects_count,
        'detections': detections
    }
```

**Detecci√≥n de poses y gestos con MediaPipe:**

```python
def detect_hands_mediapipe(self, frame: np.ndarray) -> Dict:
    """Detecta manos usando MediaPipe"""
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = self.hands.process(rgb_frame)

    hands_data = {
        'hands_count': 0,
        'hands_landmarks': [],
        'gestures': []
    }

    if results.multi_hand_landmarks:
        hands_data['hands_count'] = len(results.multi_hand_landmarks)

        for hand_landmarks in results.multi_hand_landmarks:
            landmarks = []
            for landmark in hand_landmarks.landmark:
                landmarks.extend([landmark.x, landmark.y, landmark.z])
            hands_data['hands_landmarks'].append(landmarks)

            # An√°lisis b√°sico de gestos (pu√±o cerrado vs abierto)
            gesture = self.analyze_hand_gesture(hand_landmarks.landmark)
            hands_data['gestures'].append(gesture)

    return hands_data
```

**Servidor WebSocket as√≠ncrono:**

```python
async def websocket_handler(self, websocket):
    """Maneja conexiones WebSocket"""
    self.connected_clients.add(websocket)

    # Mensaje de bienvenida
    await websocket.send(json.dumps({
        'type': 'connection',
        'message': 'üéØ Conectado al Monitor Visual 3D',
        'timestamp': time.time()
    }))

    try:
        await websocket.wait_closed()
    finally:
        self.connected_clients.discard(websocket)
```

### üîπ Frontend - Visualizaci√≥n 3D (Three.js + React)

**Hook para comunicaci√≥n WebSocket:**

```typescript
export const useWebSocket = (url: string) => {
  const [data, setData] = useState<VisualData | null>(null);
  const [connected, setConnected] = useState(false);
  const ws = useRef<WebSocket | null>(null);

  useEffect(() => {
    const connect = () => {
      ws.current = new WebSocket(url);

      ws.current.onopen = () => {
        console.log('üîå Conexi√≥n WebSocket establecida');
        setConnected(true);
      };

      ws.current.onmessage = event => {
        try {
          const receivedData = JSON.parse(event.data);
          if (receivedData.timestamp) {
            setData(receivedData);
          }
        } catch (e) {
          console.error('‚ùå Error parseando datos:', e);
        }
      };
    };

    connect();
  }, [url]);

  return { data, connected };
};
```

**Componente de escena 3D reactiva:**

```tsx
export const Scene3D: React.FC<Scene3DProps> = ({ data }) => {
  // Escalas basadas en datos en tiempo real
  const peopleScale = Math.max(0.3, data.people_count * 0.3);
  const objectsScale = Math.max(0.3, data.objects_count * 0.2);
  const movementScale = Math.max(0.2, data.movement_intensity * 2);
  const handsScale = Math.max(0.2, data.hands_count * 0.4);

  return (
    <group>
      {/* Objetos reactivos que cambian seg√∫n los datos */}
      <ReactiveObject
        position={[-3, 0, 0]}
        scale={peopleScale}
        color="#ff0080"
        type="sphere"
        intensity={data.movement_intensity}
        label={`üë• Personas: ${data.people_count}`}
        animated={true}
      />

      <ReactiveObject
        position={[3, 0, 0]}
        scale={objectsScale}
        color="#00ff80"
        type="box"
        intensity={data.movement_intensity}
        label={`üì¶ Objetos: ${data.objects_count}`}
        animated={true}
      />

      <ReactiveObject
        position={[0, 2, 0]}
        scale={movementScale}
        color="#8000ff"
        type="cone"
        intensity={data.movement_intensity}
        label={`üèÉ Movimiento: ${(data.movement_intensity * 100).toFixed(1)}%`}
        animated={true}
      />
    </group>
  );
};
```

**Objetos 3D reactivos:**

```tsx
export const ReactiveObject: React.FC<ReactiveObjectProps> = ({
  position,
  scale,
  color,
  type,
  intensity,
  animated,
}) => {
  const meshRef = useRef<THREE.Mesh>(null);

  // Animaci√≥n continua basada en intensidad
  useFrame((state, delta) => {
    if (meshRef.current && animated) {
      meshRef.current.rotation.y += delta * intensity * 2;
      meshRef.current.position.y =
        position[1] + Math.sin(state.clock.elapsedTime * 2) * intensity * 0.5;
    }
  });

  const geometry =
    type === 'sphere' ? (
      <sphereGeometry args={[1, 32, 32]} />
    ) : type === 'box' ? (
      <boxGeometry args={[1, 1, 1]} />
    ) : type === 'cone' ? (
      <coneGeometry args={[1, 2, 8]} />
    ) : (
      <cylinderGeometry args={[1, 1, 1, 8]} />
    );

  return (
    <mesh
      ref={meshRef}
      position={position}
      scale={scale}
      castShadow
      receiveShadow
    >
      {geometry}
      <meshStandardMaterial color={color} metalness={0.7} roughness={0.3} />
    </mesh>
  );
};
```

---

## üìä Resultados y Demostraci√≥n

### üé¨ Demostraci√≥n del Sistema Completo

![Monitor Visual 3D en Funcionamiento](./results/visualization_3D_monitor.gif)

_El sistema completo funcionando: detecci√≥n en tiempo real con Python (izquierda) y visualizaci√≥n 3D reactiva con Three.js (derecha)_

### üîç Caracter√≠sticas del Sistema Implementado

#### üéØ Backend Python - Detecci√≥n en Tiempo Real:

- ‚úÖ **Detecci√≥n YOLO**: Identificaci√≥n de personas y objetos con YOLOv8n
- ‚úÖ **Seguimiento de manos**: Detecci√≥n y an√°lisis de gestos con MediaPipe
- ‚úÖ **Detecci√≥n de poses**: 33 puntos clave del cuerpo humano
- ‚úÖ **An√°lisis de movimiento**: C√°lculo de intensidad de movimiento por frame
- ‚úÖ **Optimizaci√≥n de rendimiento**: Procesamiento alternado para mantener 30 FPS
- ‚úÖ **Streaming WebSocket**: Transmisi√≥n de datos en tiempo real

#### üé® Frontend Three.js - Visualizaci√≥n 3D Interactiva:

- ‚úÖ **Objetos reactivos**: Geometr√≠as que cambian escala seg√∫n detecciones
- ‚úÖ **Animaciones fluidas**: Rotaci√≥n y movimiento basado en intensidad
- ‚úÖ **Esquemas de color**: M√∫ltiples paletas (neon, pastel, vibrante, monocromo)
- ‚úÖ **Controles interactivos**: Panel de configuraci√≥n en tiempo real con Leva
- ‚úÖ **Iluminaci√≥n din√°mica**: Ambiente que responde a los datos
- ‚úÖ **Informaci√≥n en tiempo real**: Overlay con m√©tricas actualizadas

#### üåê Comunicaci√≥n y Sincronizaci√≥n:

- ‚úÖ **WebSocket bidireccional**: Latencia < 50ms entre detecci√≥n y visualizaci√≥n
- ‚úÖ **Reconexi√≥n autom√°tica**: Manejo robusto de desconexiones
- ‚úÖ **Compresi√≥n de datos**: Optimizaci√≥n del payload para mejor rendimiento
- ‚úÖ **Sincronizaci√≥n temporal**: Timestamps para coherencia de datos

### üìà M√©tricas de Rendimiento

| Componente               | M√©trica                     | Valor              |
| ------------------------ | --------------------------- | ------------------ |
| **Detecci√≥n YOLO**       | Tiempo de procesamiento     | ~30-50ms por frame |
| **MediaPipe Hands**      | Tiempo de procesamiento     | ~15-25ms por frame |
| **MediaPipe Pose**       | Tiempo de procesamiento     | ~20-30ms por frame |
| **WebSocket**            | Latencia de transmisi√≥n     | ~5-15ms            |
| **Renderizado 3D**       | FPS objetivo                | 60 FPS             |
| **Resoluci√≥n de c√°mara** | Optimizada para rendimiento | 320x240 @ 30 FPS   |

### üéÆ Interactividad y Controles

- **üéõÔ∏è Panel de Control**: Configuraci√≥n en tiempo real de escalas, colores y efectos
- **üñ±Ô∏è Controles de c√°mara**: Orbit, zoom y pan en la escena 3D
- **üîÑ Modo pantalla completa**: Visualizaci√≥n inmersiva
- **üìä M√©tricas en vivo**: Contador de FPS y estad√≠sticas de rendimiento
- **üé® Esquemas visuales**: Cambio din√°mico de paletas de colores

---

## üîß Configuraci√≥n y Uso

### üêç Requisitos del Sistema

**Backend Python:**

```bash
# Instalar dependencias de Python
pip install -r python/requirements.txt

# O instalar manualmente:
pip install opencv-python>=4.8.0 ultralytics>=8.1.0 websockets>=11.0.0 numpy>=1.24.0 mediapipe>=0.10.21
```

**Frontend Three.js:**

```bash
# Navegar al directorio del frontend
cd threejs/

# Instalar dependencias (usando Bun - m√°s r√°pido)
bun install

# O usar npm
npm install
```

### üöÄ Ejecuci√≥n del Sistema Completo

#### 1. **Iniciar el Backend Python**

```bash
cd python/
python main.py
```

_El sistema iniciar√° la c√°mara y el servidor WebSocket en el puerto 8765_

#### 2. **Iniciar el Frontend Three.js**

```bash
cd threejs/
bun dev
# O usar npm dev
```

_La aplicaci√≥n web estar√° disponible en http://localhost:5173_

#### 3. **Verificar Conexi√≥n**

- ‚úÖ El backend debe mostrar: "üéØ Monitor Visual OPTIMIZADO inicializado"
- ‚úÖ El frontend debe mostrar: "üîå Conexi√≥n WebSocket establecida"
- ‚úÖ Los datos deben fluir en tiempo real entre ambos sistemas

### üéõÔ∏è Controles y Configuraci√≥n

#### üîß Panel de Control (Leva)

- **Configuraci√≥n General**:

  - `scaleMultiplier`: Multiplicador global de escalas (0.1 - 3.0)
  - `autoRotate`: Rotaci√≥n autom√°tica de la c√°mara
  - `cameraDistance`: Distancia de la c√°mara (5 - 20)

- **Visualizaci√≥n**:

  - `enableParticles`: Activar sistema de part√≠culas
  - `showMetrics`: Mostrar m√©tricas en pantalla
  - `colorScheme`: Esquema de colores (neon, pastel, vibrante, monocromo)

- **Iluminaci√≥n**:
  - `lightIntensity`: Intensidad de luces (0.1 - 3.0)
  - `environmentPreset`: Ambiente predefinido (sunset, dawn, night, etc.)

#### ‚å®Ô∏è Controles de C√°mara

- **Rat√≥n izquierdo**: Rotar vista
- **Rat√≥n derecho**: Hacer pan
- **Rueda del rat√≥n**: Zoom in/out
- **Doble clic**: Resetear vista

#### üéØ Configuraci√≥n Backend

Modificar en `python/main.py`:

```python
# Resoluci√≥n de c√°mara (mayor = mejor calidad, menor rendimiento)
self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)   # Cambiar a 640 para HD
self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)  # Cambiar a 480 para HD

# Intervalos de procesamiento (menor = m√°s frecuente, mayor CPU)
self.yolo_interval = 3      # Procesar YOLO cada N frames
self.pose_interval = 2      # Procesar pose cada N frames
self.hands_interval = 2     # Procesar manos cada N frames
```

### üõ†Ô∏è Soluci√≥n de Problemas

#### üîç Problemas Comunes

**‚ùå Error: "No se puede conectar a WebSocket"**

- Verificar que el backend Python est√© ejecut√°ndose
- Confirmar que el puerto 8765 est√© libre
- Revisar firewall/antivirus

**‚ùå Error: "No se detecta c√°mara"**

- Verificar que la c√°mara est√© conectada y funcionando
- Probar cambiar el √≠ndice de c√°mara: `cv2.VideoCapture(1)` en lugar de `cv2.VideoCapture(0)`
- Cerrar otras aplicaciones que puedan estar usando la c√°mara

**‚ùå Rendimiento lento**

- Reducir resoluci√≥n de c√°mara
- Aumentar intervalos de procesamiento
- Cerrar aplicaciones innecesarias
- Verificar que se est√© usando GPU si est√° disponible

**‚ùå Error: "Module not found"**

- Verificar que todas las dependencias est√©n instaladas
- Usar entorno virtual de Python
- Reinstalar dependencias con `pip install --upgrade`

---

## üí¨ Reflexi√≥n Final

Este proyecto representa un logro significativo en la integraci√≥n de tecnolog√≠as de visi√≥n por computador con visualizaci√≥n 3D interactiva. Logramos crear un sistema completo que combina detecci√≥n inteligente (YOLO + MediaPipe) con visualizaci√≥n inmersiva (Three.js + React), conectados mediante WebSockets para comunicaci√≥n en tiempo real. Los aspectos clave incluyen arquitectura distribuida, optimizaci√≥n de rendimiento para mantener 30+ FPS, y la creaci√≥n de objetos 3D reactivos que responden din√°micamente a los datos de IA.

üöÄ **Innovaciones T√©cnicas**: Implementamos procesamiento alternado para ejecutar m√∫ltiples modelos de IA sin degradar rendimiento, sistema de reconexi√≥n robusto para mayor estabilidad, y controles interactivos avanzados que permiten configuraci√≥n en tiempo real. La combinaci√≥n de AsyncIO en Python con React Three Fiber demostr√≥ ser altamente efectiva para crear experiencias fluidas e inmersivas.

üåü **Impacto y Futuro**: Este sistema demuestra que es posible crear aplicaciones de IA accesibles y visualmente atractivas sin sacrificar rendimiento. Abre nuevas posibilidades para interfaces naturales, monitoreo inteligente, arte generativo y educaci√≥n interactiva. El proyecto sienta las bases para sistemas m√°s avanzados que podr√≠an incluir m√∫ltiples c√°maras, IA m√°s sofisticada con comandos de voz, realidad aumentada y colaboraci√≥n remota, demostrando que el futuro est√° en la integraci√≥n inteligente de m√∫ltiples tecnolog√≠as.
