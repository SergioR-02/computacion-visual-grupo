# ðŸ§ª Taller - Reconocimiento de Acciones Simples con DetecciÃ³n de Postura

## ðŸ“… Fecha

`2025-06-25` â€“ Taller de Reconocimiento de Acciones Simples con DetecciÃ³n de Postura

---

## ðŸŽ¯ Objetivo del Taller

Implementar el reconocimiento de acciones simples (como sentarse, levantar brazos o caminar frente a cÃ¡mara) usando MediaPipe Pose para detectar la postura corporal. El objetivo es utilizar puntos clave del cuerpo (landmarks) para interpretar la acciÃ³n y responder visual o sonoramente.

---

## ðŸ§  Conceptos Aprendidos

Lista de conceptos aplicados en este taller:

- [x] **DetecciÃ³n de Postura con MediaPipe Pose** - ExtracciÃ³n de 33 landmarks corporales en 3D.
- [x] **AnÃ¡lisis GeomÃ©trico de Landmarks** - CÃ¡lculo de Ã¡ngulos y distancias entre puntos clave para interpretar posturas.
- [x] **ClasificaciÃ³n de Acciones Basada en Reglas** - ImplementaciÃ³n de lÃ³gica para identificar acciones como "sentado" o "brazos arriba".
- [x] **Procesamiento de Video en Tiempo Real** - Uso de OpenCV para capturar, procesar y mostrar el video de la cÃ¡mara.
- [x] **Manejo de Estados y Umbrales** - DefiniciÃ³n de umbrales para activar la detecciÃ³n de acciones y evitar falsos positivos.
- [x] **RetroalimentaciÃ³n Visual** - Dibujo del esqueleto y muestra de texto en pantalla para informar la acciÃ³n detectada.
- [x] **ProgramaciÃ³n Orientada a Objetos** - EstructuraciÃ³n del cÃ³digo en clases para una mejor organizaciÃ³n y escalabilidad.

---

## ðŸ”§ Herramientas y Entornos

**Python** (Entorno principal):

- `mediapipe==0.10.9` - DetecciÃ³n de postura y landmarks corporales.
- `opencv-python==4.8.1.78` - Captura de cÃ¡mara, procesamiento de imÃ¡genes y visualizaciÃ³n.
- `numpy==1.24.3` - Operaciones numÃ©ricas para el anÃ¡lisis de landmarks.

**Sistemas de Reconocimiento**:

- **MediaPipe Pose** - Modelo de ML para la detecciÃ³n de 33 puntos clave del cuerpo humano en tiempo real.

ðŸ“Œ InstalaciÃ³n automÃ¡tica disponible con `pip install -r requirements.txt` desde el directorio `python/`.

---

## ðŸ“ Estructura del Proyecto

```
2025-06-14_taller_reconocimiento_postura_mediapipe/
â”œâ”€â”€ python/                         # Entorno principal de desarrollo
â”‚   â”œâ”€â”€ main.py                     # ðŸŽ® AplicaciÃ³n principal que ejecuta la detecciÃ³n
â”‚   â”œâ”€â”€ interactive_demo.py         # (Opcional) Script para demostraciones interactivas
â”‚   â””â”€â”€ requirements.txt            # ðŸ“¦ Lista de dependencias
â”œâ”€â”€ results/                        # ðŸ“¸ Evidencias visuales del funcionamiento
â”‚   â””â”€â”€ reconocimiento_postura_result.gif # ðŸŽ¬ GIF demostrativo
â””â”€â”€ README.md                       # ðŸ“š DocumentaciÃ³n completa
```

ðŸ“Ž Estructura limpia que separa el cÃ³digo fuente, los resultados y la documentaciÃ³n.

---

## ðŸ§ª ImplementaciÃ³n

### ðŸ”¹ Flujo de Reconocimiento de Acciones

El sistema opera siguiendo un flujo de trabajo claro para identificar acciones a partir del video:

1.  **Captura de Video**: Se utiliza OpenCV para capturar el feed de la cÃ¡mara web cuadro por cuadro.
2.  **DetecciÃ³n de Postura**: Cada cuadro se procesa con MediaPipe Pose para detectar los 33 landmarks corporales.
3.  **AnÃ¡lisis de Landmarks**: Si se detecta una postura, se extraen las coordenadas de los puntos clave necesarios.
4.  **ClasificaciÃ³n de AcciÃ³n**: Se aplica una lÃ³gica basada en reglas y umbrales para determinar si se estÃ¡ realizando una acciÃ³n conocida.
5.  **VisualizaciÃ³n**: El cuadro original se anota con el esqueleto de la postura y el nombre de la acciÃ³n detectada, y se muestra en pantalla.

### ðŸ”¹ Acciones Implementadas

| AcciÃ³n                    | LÃ³gica de DetecciÃ³n                                                                                                       |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **Brazos Arriba**         | Se verifica si las coordenadas Y de las muÃ±ecas (`15`, `16`) estÃ¡n por encima de las de los hombros (`11`, `12`).         |
| **Sentado**               | Se calcula el Ã¡ngulo de las rodillas y las caderas. Si ambos Ã¡ngulos son inferiores a 110 grados, se considera "sentado". |
| **Caminando en el sitio** | Se monitorea la variaciÃ³n vertical de los tobillos (`27`, `28`) para detectar un movimiento alternado y rÃ­tmico.          |
| **InclinaciÃ³n Lateral**   | Se mide la pendiente de la lÃ­nea formada por los hombros. Si supera un umbral, se detecta una inclinaciÃ³n.                |

### ðŸ”¹ CÃ³digo Relevante

**CÃ¡lculo de Ã¡ngulo entre tres puntos**:

```python
def calcular_angulo(a, b, c):
    """Calcula el Ã¡ngulo entre tres puntos (en grados)."""
    a = np.array(a)  # Primer punto
    b = np.array(b)  # Punto medio (vÃ©rtice)
    c = np.array(c)  # Tercer punto

    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])
    angle = np.abs(radians * 180.0 / np.pi)

    if angle > 180.0:
        angle = 360 - angle

    return angle
```

**LÃ³gica principal de detecciÃ³n**:

```python
def detectar_accion(landmarks):
    """Clasifica una acciÃ³n basada en la configuraciÃ³n de landmarks."""
    # Extraer landmarks relevantes
    hombro_izq = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]
    codo_izq = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]
    muÃ±eca_izq = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x, landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]

    # Calcular Ã¡ngulo del codo izquierdo como ejemplo
    angulo = calcular_angulo(hombro_izq, codo_izq, muÃ±eca_izq)

    # LÃ³gica de clasificaciÃ³n (ejemplo simple)
    if angulo > 160:
        return "Brazo Izquierdo Extendido"
    elif angulo < 40:
        return "Brazo Izquierdo Flexionado"

    # ... LÃ³gica para otras acciones como "Sentado" ...

    return "Neutral"
```

---

## ðŸ“Š Resultados Visuales

### ðŸ“Œ GIF Demostrativo del Funcionamiento

![Reconocimiento de Postura con MediaPipe](./results/reconocimiento_postura_result.gif)

**El GIF demuestra:**

- ðŸ‘¤ **DetecciÃ³n en tiempo real** de los 33 landmarks corporales.
- ðŸ¦´ **VisualizaciÃ³n del esqueleto** superpuesto en la imagen de la cÃ¡mara.
- ðŸ’ª **Reconocimiento de "Brazos Arriba"** cuando el usuario levanta ambos brazos.
- ðŸ§˜ **DetecciÃ³n de "Sentado"** cuando el usuario se sienta en una silla.
- ðŸš¶ **IdentificaciÃ³n de "Caminando"** al simular caminar en el sitio.
- ðŸ’¬ **Etiqueta de texto** en la esquina superior izquierda que muestra la acciÃ³n detectada dinÃ¡micamente.

### ðŸŽ¬ **CaracterÃ­sticas del Sistema:**

- **Alta velocidad**: El sistema procesa el video fluidamente gracias a la eficiencia de MediaPipe.
- **Robustez**: La detecciÃ³n funciona desde diferentes Ã¡ngulos y distancias moderadas.
- **Bajo consumo de recursos**: No requiere hardware especializado, funciona en CPUs modernas.
- **Extensible**: La arquitectura basada en reglas facilita la adiciÃ³n de nuevas acciones personalizadas.

---

## ðŸ§© Prompts Usados

### âœ… **Prompts de Desarrollo Utilizados**

#### 1. **Prompt Inicial de Arquitectura**

```text
"Crea un script en Python que use OpenCV para capturar video de la webcam y MediaPipe Pose para detectar y dibujar los landmarks de la postura en tiempo real. La estructura debe estar en una clase para mantener el cÃ³digo organizado."
```

**Resultado**: Clase base con el bucle principal de OpenCV y la inicializaciÃ³n de MediaPipe Pose.

#### 2. **Prompt para LÃ³gica de DetecciÃ³n**

```text
"Escribe una funciÃ³n que tome los 33 landmarks de MediaPipe Pose como entrada y determine si la persona estÃ¡ 'sentada'. Para ello, calcula el Ã¡ngulo de las articulaciones de la cadera y la rodilla. Si ambos Ã¡ngulos son menores a 110 grados, debe devolver 'Sentado'."
```

**Resultado**: Una funciÃ³n de `calcular_angulo` y la lÃ³gica especÃ­fica para detectar la acciÃ³n de sentarse.

#### 3. **Prompt para DetecciÃ³n de Brazos Arriba**

```text
"Implementa una funciÃ³n que detecte si el usuario tiene los 'brazos arriba'. La condiciÃ³n es que las coordenadas 'y' de ambas muÃ±ecas sean menores que las coordenadas 'y' de los hombros correspondientes."
```

**Resultado**: LÃ³gica simple y eficiente para comparar las posiciones verticales de muÃ±ecas y hombros.

#### 4. **Prompt para VisualizaciÃ³n de Resultados**

```text
"Modifica el script para que, ademÃ¡s de dibujar el esqueleto, muestre el nombre de la acciÃ³n detectada en la esquina superior izquierda de la ventana de video. El texto debe ser claro y visible."
```

**Resultado**: Uso de la funciÃ³n `cv2.putText` para superponer el estado actual en el fotograma de video.

#### 5. **Prompt para Estructura de Proyecto y README**

```text
"Genera una estructura de proyecto recomendada para una aplicaciÃ³n de reconocimiento de postura con Python. Incluye un archivo README.md documentando el objetivo, las herramientas, la implementaciÃ³n y los resultados, con ejemplos de cÃ³digo y un GIF demostrativo."
```

**Resultado**: Este mismo archivo `README.md` que estÃ¡s leyendo y la organizaciÃ³n de carpetas sugerida.

---

## ðŸ’¬ ReflexiÃ³n Final

**Â¿QuÃ© te pareciÃ³ traducir movimientos corporales en datos analizables?**

Es fascinante ver cÃ³mo un movimiento humano, algo que percibimos de forma tan natural, puede descomponerse en un conjunto de coordenadas y Ã¡ngulos. La capacidad de MediaPipe para abstraer la complejidad del cuerpo humano en 33 puntos clave es increÃ­blemente poderosa. Al principio, definir las "reglas" para una acciÃ³n como "sentarse" se siente como tratar de enseÃ±arle a una mÃ¡quina a entender un concepto muy humano, lo cual es un desafÃ­o muy interesante.

**Â¿QuÃ© parte fue mÃ¡s compleja o interesante?**

La parte mÃ¡s compleja fue definir umbrales que funcionaran de manera robusta. Un umbral de Ã¡ngulo que funciona para una persona puede no ser ideal para otra, o puede variar segÃºn el Ã¡ngulo de la cÃ¡mara. Esto requiere ajuste fino y pruebas. Lo mÃ¡s interesante fue, sin duda, el momento "eureka" en que la lÃ³gica funciona y la etiqueta en la pantalla refleja correctamente el movimiento que estÃ¡s realizando. Es una conexiÃ³n muy directa entre tu acciÃ³n fÃ­sica y la respuesta del programa.

**Â¿QuÃ© mejorarÃ­as o quÃ© aplicarÃ­as en futuros proyectos?**

Para mejorar, en lugar de reglas fijas, se podrÃ­a entrenar un modelo de aprendizaje automÃ¡tico (como una red neuronal recurrente o LSTM) que aprenda a reconocer acciones a partir de secuencias de landmarks. Esto permitirÃ­a detectar acciones mÃ¡s complejas y dinÃ¡micas (como saludar, aplaudir o diferentes tipos de ejercicios). Esta misma tecnologÃ­a se puede aplicar a proyectos de fisioterapia virtual, control de videojuegos mediante el cuerpo (exergaming), anÃ¡lisis de rendimiento deportivo o sistemas de seguridad para detectar caÃ­das en personas mayores. La base creada en este taller es un excelente punto de partida para todas esas aplicaciones.

---


